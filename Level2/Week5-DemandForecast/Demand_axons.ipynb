{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW2Wg39xiP_c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import polars as pl\n",
        "import datetime\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set options to display all rows, all columns, and full column width\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', 1000) # Adjust as needed for your console/display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcGbKAhyjWBX"
      },
      "outputs": [],
      "source": [
        "transaction1 = pl.read_parquet('/project/ai901504-ai0004/501641_Big/week3/demand-forecasting-non-duck-product/problem1_filter_transaction_price_1.parquet')\n",
        "# transaction2 = pl.read_parquet('/project/ai901504-ai0004/501641_Big/week3/demand-forecasting-non-duck-product/problem1_filter_transaction_price_2.parquet')\n",
        "event_date = pl.read_parquet('/project/ai901504-ai0004/501641_Big/week3/demand-forecasting-non-duck-product/event_date.parquet')\n",
        "# price = pl.read_parquet('/content/kaggle_competition/problem1_price.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JP7-DB_t_jY"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es8vcs3JTXlu"
      },
      "source": [
        "### price_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "txoJITz0TZl0",
        "outputId": "ee102003-450a-419c-dc25-817a2b199bfc"
      },
      "outputs": [],
      "source": [
        "# price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYgSpQrXuFTL"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "eM_HZuC-kxH4",
        "outputId": "9b141120-27c8-4361-e2f3-5bbe6ebe7d2d"
      },
      "outputs": [],
      "source": [
        "transaction1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NByALk_puIIi",
        "outputId": "6db1de0f-d5ec-4736-9d71-9950c172eef9"
      },
      "outputs": [],
      "source": [
        "pl.Config.set_tbl_cols(-1)\n",
        "print(transaction1.null_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfgnkFtfh9Aq",
        "outputId": "1c086ebc-cfaf-472d-fd98-93e6028b1349"
      },
      "outputs": [],
      "source": [
        "print(transaction1.select(pl.col(\"WarehouseBKey\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"CustomerBKey\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"CustomerArea3NameLocal\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"ProductForPlan1\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"ProductForPlan8\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"ProductForPlan10\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"MODEL_4\").n_unique()).item())\n",
        "print(transaction1.select(pl.col(\"region\").n_unique()).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7UmtWjtemSh"
      },
      "source": [
        "### Group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miMd9us7jpGO"
      },
      "outputs": [],
      "source": [
        "groupmodel_4 = transaction1.group_by(['MODEL_4', 'ProductForPlan10']).agg([\n",
        "    pl.col('OrderWeight').sum(),\n",
        "    pl.col('OrderWeight').mean().alias(\"OrderWeight_mean\"),\n",
        "    pl.col('OrderWeight').std().alias(\"OrderWeight_std\"),\n",
        "    pl.col('OrderWeight').median().alias(\"OrderWeight_median\"),\n",
        "    pl.col('OrderWeight').mode().alias(\"OrderWeight_mode\"),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "yreY9teOkT0v",
        "outputId": "4a3b04f9-e24a-49f6-d5f2-9fbb0022090b"
      },
      "outputs": [],
      "source": [
        "groupmodel_4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU-7fowRy1Sx"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25-GJ1R8y0qy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def random_person_history_on_one_item(df):\n",
        "    # Get unique list of customers\n",
        "    list_people = df.select(\"CustomerBKey\").unique().to_series().to_list()\n",
        "    random_person = random.choice(list_people)\n",
        "\n",
        "    # Get all transactions for that person\n",
        "    random_person_history = df.filter(pl.col(\"CustomerBKey\") == random_person)\n",
        "\n",
        "    # Get unique list of products for that person\n",
        "    list_items = random_person_history.select(\"ProductBKey\").unique().to_series().to_list()\n",
        "    random_item = random.choice(list_items)\n",
        "\n",
        "    # Filter transactions for that person on one product\n",
        "    person_history_on_item = random_person_history.filter(pl.col(\"ProductBKey\") == random_item)\n",
        "\n",
        "    # Visualize\n",
        "    person_history_on_item = person_history_on_item.sort(\"OrderDate\")\n",
        "    df_plot = person_history_on_item.select([\"OrderDate\", \"OrderWeight\"]).to_pandas()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_plot[\"OrderDate\"], df_plot[\"OrderWeight\"], marker='o', linestyle='-')\n",
        "    plt.title(f\"OrderWeight Over Time\\nCustomerBKey: {random_person}, ProductBKey: {random_item}\")\n",
        "    plt.xlabel(\"OrderDate\")\n",
        "    plt.ylabel(\"OrderWeight\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return person_history_on_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "4O3Gd_IV7rgT",
        "outputId": "f668266f-1e98-4be2-c2a5-53562557dcdc"
      },
      "outputs": [],
      "source": [
        "random_person = random_person_history_on_one_item(transaction1)\n",
        "print(random_person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXaHKJc1k8YE"
      },
      "outputs": [],
      "source": [
        "def visualize_group_byproduct(df, column=str):\n",
        "    # Step 1: Get list of unique product categories\n",
        "    list_product = df.select(\"ProductForPlan10\").unique().to_series().to_list()\n",
        "\n",
        "    # Step 2: Pick one randomly\n",
        "    random_product = random.choice(list_product)\n",
        "\n",
        "    # Step 3: Filter rows for that product category\n",
        "    random_product_sale = df.filter(pl.col(\"ProductForPlan10\") == random_product)\n",
        "\n",
        "    # Step 4: Prepare data for plotting\n",
        "    product_names = random_product_sale[\"MODEL_4\"].to_list()\n",
        "    weights = random_product_sale[column].to_list()\n",
        "\n",
        "    # Step 5: Create Plotly bar chart\n",
        "    fig = px.bar(\n",
        "        x=weights,\n",
        "        y=product_names,\n",
        "        orientation=\"h\",\n",
        "        labels={\"x\": \"Total OrderWeight\", \"y\": \"District (MODEL_4)\"},\n",
        "        title=f\"Sales by District for Product Category: {random_product}\",\n",
        "    )\n",
        "\n",
        "    # Optional: Highest value on top\n",
        "    fig.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
        "\n",
        "    # Show the interactive plot\n",
        "    fig.show()\n",
        "\n",
        "    return random_product_sale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeneMOVupAg9"
      },
      "outputs": [],
      "source": [
        "def visualize_group_byregion(df, column=str):\n",
        "    # Step 1: Get list of unique product categories\n",
        "    list_product = df.select(\"MODEL_4\").unique().to_series().to_list()\n",
        "\n",
        "    # Step 2: Pick one randomly\n",
        "    random_product = random.choice(list_product)\n",
        "\n",
        "    # Step 3: Filter rows for that product category\n",
        "    random_product_sale = df.filter(pl.col(\"MODEL_4\") == random_product)\n",
        "\n",
        "    # Step 4: Prepare data for plotting\n",
        "    product_names = random_product_sale[\"ProductForPlan10\"].to_list()\n",
        "    weights = random_product_sale[column].to_list()\n",
        "\n",
        "    # Step 5: Create Plotly bar chart\n",
        "    fig = px.bar(\n",
        "        x=weights,\n",
        "        y=product_names,\n",
        "        orientation=\"h\",\n",
        "        labels={\"x\": \"Total OrderWeight\", \"y\": \"Item (Plan 10)\"},\n",
        "        title=f\"Sales by Product for district : {random_product}\",\n",
        "    )\n",
        "\n",
        "    # Optional: Highest value on top\n",
        "    fig.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
        "\n",
        "    # Show the interactive plot\n",
        "    fig.show()\n",
        "\n",
        "    return random_product_sale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u3NxqtpLmfb4",
        "outputId": "ae25b0fe-6f58-4ba0-dab1-7972fa3b0006"
      },
      "outputs": [],
      "source": [
        "random_item = visualize_group_byproduct(groupmodel_4, 'OrderWeight_mean')\n",
        "print(random_item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "Ua74ZUHlpKog",
        "outputId": "04f852fe-e1b6-4b35-e051-2a38f9104c36"
      },
      "outputs": [],
      "source": [
        "random_district = visualize_group_byregion(groupmodel_4, 'OrderWeight_mean')\n",
        "print(random_district)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37STyTVI3ue4"
      },
      "source": [
        "### à¸ à¸²à¸„à¹ƒà¸•à¹‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC07Erjs5xD0",
        "outputId": "03537c26-0726-413f-91ed-72dfe19b6209"
      },
      "outputs": [],
      "source": [
        "print(transaction['region'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBnxsf1v4r9V"
      },
      "outputs": [],
      "source": [
        "transaction = transaction.filter(pl.col(\"region\") == \"à¸ à¸²à¸„à¹ƒà¸•à¹‰\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "uIicACmQ6DIr",
        "outputId": "c183c0fb-5fdb-43d4-8cc5-1b6f25351a60"
      },
      "outputs": [],
      "source": [
        "transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SstlN2L46QLc"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def visual_stat_on_plan10(df: pl.DataFrame) -> None:\n",
        "    # Step 1: Randomly select a region (MODEL_4)\n",
        "    list_region = df.select(\"MODEL_4\").unique().to_series().to_list()\n",
        "    random_region = random.choice(list_region)\n",
        "\n",
        "    # Step 2: Filter rows for that region\n",
        "    region_df = df.filter(pl.col(\"MODEL_4\") == random_region)\n",
        "\n",
        "    # Step 3: Randomly select a product (ProductForPlan10) from that region\n",
        "    product_ids = region_df.select(\"ProductForPlan10\").unique().to_series().to_list()\n",
        "    random_product = random.choice(product_ids)\n",
        "\n",
        "    # Step 4: Filter for that product\n",
        "    product_df = region_df.filter(pl.col(\"ProductForPlan10\") == random_product)\n",
        "\n",
        "    # Step 5: Aggregate statistics by date\n",
        "    daily_stats = (\n",
        "        product_df.group_by(\"OrderDate\").agg([\n",
        "            pl.col(\"OrderWeight\").sum().alias(\"daily_total\"),\n",
        "            pl.col(\"OrderWeight\").mean().alias(\"daily_mean\"),\n",
        "            pl.col(\"OrderWeight\").median().alias(\"daily_median\"),\n",
        "            pl.col(\"OrderWeight\").std().alias(\"daily_std\"),\n",
        "            pl.col(\"OrderWeight\").max().alias(\"daily_max\"),\n",
        "            pl.col(\"OrderWeight\").min().alias(\"daily_min\"),\n",
        "            pl.count().alias(\"order_count\"),\n",
        "        ])\n",
        "        .sort(\"OrderDate\")\n",
        "    )\n",
        "\n",
        "    # Convert to pandas for Plotly\n",
        "    stats_pd = daily_stats.to_pandas()\n",
        "\n",
        "    # Step 6: Plot using Plotly\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=stats_pd[\"OrderDate\"],\n",
        "        y=stats_pd[\"daily_total\"],\n",
        "        mode='lines+markers',\n",
        "        name='Total Sales',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=stats_pd[\"OrderDate\"],\n",
        "        y=stats_pd[\"daily_mean\"],\n",
        "        mode='lines+markers',\n",
        "        name='Mean Sales',\n",
        "        line=dict(color='orange', dash='dash')\n",
        "    ))\n",
        "\n",
        "    # Add shaded area for Â±1 standard deviation\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=stats_pd[\"OrderDate\"],\n",
        "        y=stats_pd[\"daily_mean\"] + stats_pd[\"daily_std\"],\n",
        "        mode='lines',\n",
        "        name='+1 Std Dev',\n",
        "        line=dict(width=0),\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=stats_pd[\"OrderDate\"],\n",
        "        y=stats_pd[\"daily_mean\"] - stats_pd[\"daily_std\"],\n",
        "        mode='lines',\n",
        "        fill='tonexty',\n",
        "        name='Â±1 Std Dev',\n",
        "        fillcolor='rgba(0, 0, 255, 0.1)',\n",
        "        line=dict(width=0),\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=f\"ðŸ“¦ Product {random_product} in Region {random_region}\",\n",
        "        xaxis_title=\"Order Date\",\n",
        "        yaxis_title=\"OrderWeight\",\n",
        "        legend=dict(x=0, y=1.1, orientation=\"h\"),\n",
        "        hovermode=\"x unified\",\n",
        "        template=\"plotly_white\",\n",
        "        margin=dict(l=40, r=40, t=60, b=40),\n",
        "        height=500,\n",
        "        width=1000\n",
        "    )\n",
        "\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "ajcOFi2A95Uk",
        "outputId": "e7f4d515-415a-4b76-84ac-99cd15204692"
      },
      "outputs": [],
      "source": [
        "visual_stat_on_plan10(transaction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JjrFsVzUwst"
      },
      "source": [
        "# Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2xmkaHxgCrU"
      },
      "outputs": [],
      "source": [
        "transaction1 = transaction1.drop([\"key_cv_p\", \"WeekMon\", \"DeliveryDate\", \"WarehouseBKey\", \"DeliveryWeight\",\n",
        "                                  \"price_key\", \"cls_prd_price\", \"prd_price\", \"__index_level_0__\",\n",
        "                                  \"CLASS_PRICE\", \"CustomerArea3NameLocal\",\n",
        "                                  \"WeightUnit\", \"region\", 'p10_avg_price'])\n",
        "''' transaction2 = transaction2.drop([\"key_cv_p\", \"WeekMon\", \"DeliveryDate\", \"WarehouseBKey\", \"DeliveryWeight\",\n",
        "                                  \"price_key\", \"cls_prd_price\", \"prd_price\", \"__index_level_0__\",\n",
        "                                  \"CLASS_PRICE\", \"CustomerArea3NameLocal\",\n",
        "                                  \"WeightUnit\", \"region\", 'p10_avg_price']) '''\n",
        "\n",
        "#\"region\"\n",
        "#'key_cv_p'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transaction1 = transaction1.group_by(['CustomerBKey', 'OrderDate', 'ProductBKey']).agg([\n",
        "    pl.col('OrderWeight').sum().alias('OrderWeight'),\n",
        "    pl.col('ProductForPlan1').first(),\n",
        "    pl.col('ProductForPlan8').first(),\n",
        "    pl.col('ProductForPlan10').first(),\n",
        "    pl.col('MODEL_4').first()\n",
        "])\n",
        "\n",
        "'''transaction2 = transaction2.group_by(['CustomerBKey', 'OrderDate', 'ProductBKey']).agg([\n",
        "    pl.col('OrderWeight').sum().alias('OrderWeight'),\n",
        "    pl.col('ProductForPlan1').first(),\n",
        "    pl.col('ProductForPlan8').first(),\n",
        "    pl.col('ProductForPlan10').first(),\n",
        "    pl.col('MODEL_4').first()\n",
        "])'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk-zqj12ZFgU"
      },
      "source": [
        "### Time_Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTqT50kKUzV_",
        "outputId": "e6057510-d4c3-4f04-842d-662e7ca86a2f"
      },
      "outputs": [],
      "source": [
        "print(f\"Min date: {transaction1['OrderDate'].min()}\")\n",
        "print(f\"Max date: {transaction1['OrderDate'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tsZLhcvXVgl"
      },
      "outputs": [],
      "source": [
        "# transaction1 = transaction1.filter(\n",
        "#     (pl.col(\"OrderDate\").is_between(pl.lit(\"2023-01-01\").cast(pl.Date), pl.lit(\"2023-05-01\").cast(pl.Date))) |\n",
        "#     (pl.col(\"OrderDate\").is_between(pl.lit(\"2024-01-01\").cast(pl.Date), pl.lit(\"2024-05-01\").cast(pl.Date))) |\n",
        "#     (pl.col(\"OrderDate\").is_between(pl.lit(\"2025-01-01\").cast(pl.Date), pl.lit(\"2025-04-03\").cast(pl.Date)))\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzQckeM4YFf8",
        "outputId": "23b769db-ce3c-4250-88ad-e2a83c839ef5"
      },
      "outputs": [],
      "source": [
        "'''transaction3 = transaction2.filter(\n",
        "    (pl.col(\"OrderDate\").is_between(pl.lit(\"2023-02-01\").cast(pl.Date), pl.lit(\"2023-05-01\").cast(pl.Date)))\n",
        ")'''\n",
        "\n",
        "transaction2 = transaction1.filter(\n",
        "    (pl.col(\"OrderDate\").is_between(pl.lit(\"2024-02-01\").cast(pl.Date), pl.lit(\"2024-05-01\").cast(pl.Date)))\n",
        ")\n",
        "\n",
        "transaction1 = transaction1.filter(\n",
        "    (pl.col(\"OrderDate\").is_between(pl.lit(\"2025-02-01\").cast(pl.Date), pl.lit(\"2025-04-03\").cast(pl.Date)))\n",
        ")\n",
        "\n",
        "print(f\"Min date1: {transaction1['OrderDate'].min()}\")\n",
        "print(f\"Max date1: {transaction1['OrderDate'].max()}\")\n",
        "\n",
        "print(f\"Min date2: {transaction2['OrderDate'].min()}\")\n",
        "print(f\"Max date2: {transaction2['OrderDate'].max()}\")\n",
        "\n",
        "'''print(f\"Min date3: {transaction3['OrderDate'].min()}\")\n",
        "print(f\"Max date3: {transaction3['OrderDate'].max()}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IMM25CcouRFG",
        "outputId": "02f9a377-d2e1-46d6-fa14-a688041b6003"
      },
      "outputs": [],
      "source": [
        "transaction1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter_people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pl.read_csv('/project/ai901504-ai0004/501641_Big/week3/demand-forecasting-non-duck-product/problem1_test.csv',\n",
        "                      schema_overrides={'ProductBKey': pl.Utf8})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_people = test_df['CustomerBKey'].unique()\n",
        "list_item = test_df['ProductBKey'].unique()\n",
        "\n",
        "print(transaction1.filter(\n",
        "    pl.col(\"CustomerBKey\").is_in(list_people)\n",
        ").select(\"CustomerBKey\").unique().height)\n",
        "\n",
        "print(transaction2.filter(\n",
        "    pl.col(\"CustomerBKey\").is_in(list_people)\n",
        ").select(\"CustomerBKey\").unique().height)\n",
        "\n",
        "'''print(transaction3.filter(\n",
        "    pl.col(\"CustomerBKey\").is_in(list_people)\n",
        ").select(\"CustomerBKey\").unique().height)'''\n",
        "\n",
        "transaction1 = transaction1.filter(pl.col(\"CustomerBKey\").is_in(list_people))\n",
        "transaction2 = transaction2.filter(pl.col(\"CustomerBKey\").is_in(list_people))\n",
        "# transaction3 = transaction3.filter(pl.col(\"CustomerBKey\").is_in(list_people))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBhZ1MTosj-"
      },
      "source": [
        "### Time_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEtCExoQ3U4J",
        "outputId": "a7f0dcf6-c71b-4f80-a9cf-3ec65fd359eb"
      },
      "outputs": [],
      "source": [
        "print(transaction1.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJIo5d1Vovt_",
        "outputId": "c539dfd1-59d9-455b-8da6-2c21ee7acdf0"
      },
      "outputs": [],
      "source": [
        "def to_timeseries(df_sales: pl.DataFrame) -> pl.DataFrame:\n",
        "    # 1. Get min and max date\n",
        "    min_date = df_sales.select(pl.col(\"OrderDate\").min()).item()\n",
        "    max_date = df_sales.select(pl.col(\"OrderDate\").max()).item()\n",
        "\n",
        "    # 2. Create calendar\n",
        "    calendar_df = pl.DataFrame({\n",
        "        \"OrderDate\": pl.date_range(min_date, max_date, interval=\"1d\", eager=True)\n",
        "    })\n",
        "\n",
        "    # 3. Get frequent Customer-Product pairs\n",
        "    pairs = (\n",
        "        df_sales\n",
        "        .group_by([\"CustomerBKey\", \"ProductForPlan10\", 'ProductForPlan1', 'ProductBKey', 'MODEL_4', 'ProductForPlan8'])\n",
        "        .len()\n",
        "        .filter(pl.col(\"len\") > 5)\n",
        "        .select([\"CustomerBKey\", \"ProductForPlan10\", 'ProductForPlan1', 'ProductBKey', 'MODEL_4', 'ProductForPlan8'])\n",
        "    )\n",
        "\n",
        "    # 4. Cross join pairs with dates to build time series base\n",
        "    base_df = pairs.join(calendar_df, how=\"cross\")\n",
        "\n",
        "    sales_data = df_sales.select([\n",
        "        'OrderDate', 'CustomerBKey', 'ProductForPlan10', 'OrderWeight'\n",
        "    ]).with_columns(pl.col(\"OrderDate\").cast(pl.Date))\n",
        "\n",
        "    base_df = base_df.join(sales_data, on=[\"OrderDate\", \"CustomerBKey\", \"ProductForPlan10\"], how=\"left\")\n",
        "\n",
        "    # 6. Fill missing OrderWeight with 0\n",
        "    base_df = base_df.with_columns(\n",
        "        pl.col(\"OrderWeight\").fill_null(0.0).cast(pl.Float32)\n",
        "    )\n",
        "\n",
        "    return base_df\n",
        "\n",
        "def cutout_zeros(df: pl.DataFrame, col: str = 'OrderWeight', fraction: float = 0.5, seed: int = 42) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Randomly removes a fraction of rows where `col == 0`.\n",
        "\n",
        "    Args:\n",
        "        df (pl.DataFrame): The Polars DataFrame.\n",
        "        col (str): Column name to evaluate (usually \"sales\").\n",
        "        frac (float): Fraction (0.0 to 1.0) of zero-value rows to keep.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Filtered DataFrame with reduced zero-value rows.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Count stats\n",
        "    zero_rows = df.filter(pl.col(col) == 0)\n",
        "    non_zero_rows = df.filter(pl.col(col) > 0)\n",
        "\n",
        "    print(f\"Original rows with {col} == 0: {len(zero_rows)}\")\n",
        "    print(f\"Original rows with {col} >  0: {len(non_zero_rows)}\")\n",
        "\n",
        "\n",
        "    # Sample from zero rows\n",
        "    n_samples = int(zero_rows.height * fraction)\n",
        "    zero_sampled = zero_rows.sample(n=n_samples, seed=seed)\n",
        "\n",
        "    # Combine back\n",
        "    result = pl.concat([non_zero_rows, zero_sampled]).rechunk()\n",
        "    print(f\"After cutout: {result.filter(pl.col(col) == 0).height} rows with {col} == 0\")\n",
        "    print(f\"Total rows after cutout: {result.height}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "transaction1 = to_timeseries(transaction1)\n",
        "transaction1 = cutout_zeros(transaction1, col='OrderWeight', fraction=1, seed=42)\n",
        "\n",
        "transaction2 = to_timeseries(transaction2)\n",
        "transaction2 = cutout_zeros(transaction2, col='OrderWeight', fraction=1, seed=42)\n",
        "\n",
        "'''transaction3 = to_timeseries(transaction3)\n",
        "transaction3 = cutout_zeros(transaction3, col='OrderWeight', fraction=1, seed=42)'''\n",
        "\n",
        "transaction = pl.concat([transaction1, transaction2]).rechunk()\n",
        "del transaction1, transaction2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNFCVEPqwRUG",
        "outputId": "10604e88-bd6d-4dd9-cd7f-866dde961634"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW1GrkJExBbx"
      },
      "outputs": [],
      "source": [
        "transaction = transaction.with_columns([\n",
        "    pl.col(\"OrderDate\").dt.weekday().alias(\"day_of_week\"),  # 0 = Monday\n",
        "    pl.col(\"OrderDate\").dt.month().alias(\"month\"),\n",
        "    pl.when(((pl.col(\"OrderDate\").dt.day() - 1) // 7 + 1) > 4)\n",
        "      .then(4)\n",
        "      .otherwise((pl.col(\"OrderDate\").dt.day() - 1) // 7 + 1)\n",
        "      .alias(\"week\"),\n",
        "    pl.col(\"OrderDate\").dt.year().alias(\"year\")  \n",
        "])\n",
        "\n",
        "transaction = transaction.with_columns([\n",
        "    pl.col(\"day_of_week\").map_elements(lambda x: np.sin(2 * np.pi * x / 7)).alias(\"day_of_week_sin\"),\n",
        "    pl.col(\"day_of_week\").map_elements(lambda x: np.cos(2 * np.pi * x / 7)).alias(\"day_of_week_cos\")\n",
        "])\n",
        "\n",
        "transaction = transaction.sort([\"CustomerBKey\", \"ProductBKey\", \"OrderDate\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### People_behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transaction.head(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_stats0 = transaction.group_by(['CustomerBKey', 'ProductBKey', 'month', 'year']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_people_last_month_mean'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_people_last_month_std'),\n",
        "    pl.col('OrderWeight').max().alias('OrderWeight_people_last_month_max'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).min().alias('OrderWeight_people_last_month_min'),\n",
        "\n",
        "    # Statistics for OrderWeight > 0 only\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).mean().alias('OrderWeight_people_last_month_mean_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).median().alias('OrderWeight_people_last_month_median_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).std().alias('OrderWeight_people_last_month_std_gt0')\n",
        "])\n",
        "print(group_stats0) #stat data of people in each week at that time of the year\n",
        "\n",
        "null_counts = group_stats0.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats0.columns\n",
        "])\n",
        "print(null_counts)\n",
        "\n",
        "group_stats0 = group_stats0.with_columns([\n",
        "    pl.col(\"OrderWeight_people_last_month_mean\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_std\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_max\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_min\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_mean_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_median_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_std_gt0\").fill_null(0)\n",
        "])\n",
        "\n",
        "group_stats0 = group_stats0.with_columns(\n",
        "    (pl.col(\"month\") + 1).alias(\"month\")\n",
        ")\n",
        "\n",
        "def apply_stats0(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['CustomerBKey', 'ProductBKey', 'month', 'year'], how='left')\n",
        "\n",
        "    # Optional: Fill nulls after join if any (could result from missing group_stats rows)\n",
        "    df = df.fill_null(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats0(transaction, group_stats0)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_stats4 = transaction.group_by(['CustomerBKey', 'ProductBKey', 'month', 'year', 'week']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_people_last_month_week_mean'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_people_last_month_week_std'),\n",
        "    pl.col('OrderWeight').max().alias('OrderWeight_people_last_month_week_max'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).min().alias('OrderWeight_people_last_month_week_min'),\n",
        "\n",
        "    # Statistics for OrderWeight > 0 only\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).mean().alias('OrderWeight_people_last_month_week_mean_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).median().alias('OrderWeight_people_last_month_week_median_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).std().alias('OrderWeight_people_last_month_week_std_gt0')\n",
        "])\n",
        "print(group_stats4) #stat data of people in each week at that time of the year\n",
        "\n",
        "null_counts = group_stats4.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats4.columns\n",
        "])\n",
        "print(null_counts)\n",
        "\n",
        "group_stats4 = group_stats4.with_columns([\n",
        "    pl.col(\"OrderWeight_people_last_month_week_mean\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_std\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_max\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_min\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_mean_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_median_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_last_month_week_std_gt0\").fill_null(0)\n",
        "])\n",
        "\n",
        "group_stats4 = group_stats4.with_columns(\n",
        "    (pl.col(\"month\") + 1).alias(\"month\")\n",
        ")\n",
        "\n",
        "group_stats4 = group_stats4.with_columns([\n",
        "    ((pl.col(\"week\") % 4) + 1).alias(\"week\"),  # Shift week forward, wrap around after 4\n",
        "    (pl.when(pl.col(\"week\") == 1)\n",
        "     .then(pl.col(\"month\") + 1)\n",
        "     .otherwise(pl.col(\"month\"))).alias(\"month\")\n",
        "])\n",
        "\n",
        "def apply_stats4(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['CustomerBKey', 'ProductBKey', 'month', 'year', 'week'], how='left')\n",
        "\n",
        "    # Drop rows where 'month' is 2 (February) because there's no previous month data (i.e., no stats from Jan)\n",
        "    df = df.filter(pl.col(\"month\") != 2)\n",
        "\n",
        "    # Optional: Fill nulls after join if any (could result from missing group_stats rows)\n",
        "    df = df.fill_null(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats4(transaction, group_stats4)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_stats1 = transaction.group_by(['CustomerBKey', 'ProductForPlan1', 'ProductForPlan8', 'month', 'week']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_people_mean'),\n",
        "    pl.col('OrderWeight').median().alias('OrderWeight_people_median'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_people_std'),\n",
        "    pl.col('OrderWeight').max().alias('OrderWeight_people_max'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).min().alias('OrderWeight_people_min'),\n",
        "\n",
        "    # Statistics for OrderWeight > 0 only\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).mean().alias('OrderWeight_people_mean_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).median().alias('OrderWeight_people_median_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).std().alias('OrderWeight_people_std_gt0')\n",
        "])\n",
        "print(group_stats1) #stat data of people in each week at that time of the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_counts = group_stats1.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats1.columns\n",
        "])\n",
        "print(null_counts)\n",
        "group_stats1 = group_stats1.with_columns([\n",
        "    pl.col(\"OrderWeight_people_std\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_min\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_mean_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_median_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_std_gt0\").fill_null(0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_stats1(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['CustomerBKey', 'ProductForPlan1', 'ProductForPlan8', 'month', 'week'], how='left')\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats1(transaction, group_stats1)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_stats2 = transaction.group_by(['CustomerBKey', 'ProductForPlan1', 'ProductForPlan8', 'month']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_people_mean_inmonth'),\n",
        "    pl.col('OrderWeight').median().alias('OrderWeight_people_median_inmonth'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_people_std_inmonth')\n",
        "])\n",
        "print(group_stats2) #stat data of people in each week at that time of the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_counts = group_stats2.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats2.columns\n",
        "])\n",
        "print(null_counts)\n",
        "group_stats2 = group_stats2.with_columns([\n",
        "    pl.col(\"OrderWeight_people_mean_inmonth\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_median_inmonth\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_std_inmonth\").fill_null(0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_stats2(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['CustomerBKey', 'ProductForPlan1', 'ProductForPlan8', 'month'], how='left')\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats2(transaction, group_stats2)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_stats3 = transaction.group_by(['CustomerBKey', 'ProductBKey', 'day_of_week']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_people_cycle_mean'),\n",
        "    pl.col('OrderWeight').median().alias('OrderWeight_people_cycle_median'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_people_cycle_std'),\n",
        "    pl.col('OrderWeight').max().alias('OrderWeight_people_cycle_max'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).min().alias('OrderWeight_people_cycle_min'),\n",
        "\n",
        "    # Statistics for OrderWeight > 0 only\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).mean().alias('OrderWeight_people_cycle_mean_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).median().alias('OrderWeight_people_cycle_median_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).std().alias('OrderWeight_people_cycle_std_gt0')\n",
        "])\n",
        "print(group_stats3) #stat data of people in each week at that time of the year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "null_counts = group_stats3.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats3.columns\n",
        "])\n",
        "print(null_counts)\n",
        "group_stats3 = group_stats3.with_columns([\n",
        "    pl.col(\"OrderWeight_people_cycle_mean\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_median\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_std\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_max\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_min\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_mean_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_median_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_people_cycle_std_gt0\").fill_null(0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_stats3(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['CustomerBKey', 'ProductBKey', 'day_of_week'], how='left')\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats3(transaction, group_stats3)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4cGzrV76R3g"
      },
      "source": [
        "### Statistic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VVhdGzXI6U0e",
        "outputId": "1925732b-9edc-4565-e334-905a45864ba5"
      },
      "outputs": [],
      "source": [
        "transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40YmiHqrBXty"
      },
      "outputs": [],
      "source": [
        "group_stats = transaction.group_by(['ProductForPlan10', 'MODEL_4', 'month', 'week']).agg([\n",
        "    # Regular statistics\n",
        "    pl.col('OrderWeight').mean().alias('OrderWeight_mean'),\n",
        "    pl.col('OrderWeight').median().alias('OrderWeight_median'),\n",
        "    pl.col('OrderWeight').std().alias('OrderWeight_std'),\n",
        "    pl.col('OrderWeight').max().alias('OrderWeight_max'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).min().alias('OrderWeight_min'),\n",
        "\n",
        "    # Statistics for OrderWeight > 0 only\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).mean().alias('OrderWeight_mean_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).median().alias('OrderWeight_median_gt0'),\n",
        "    pl.col('OrderWeight').filter(pl.col('OrderWeight') > 0).std().alias('OrderWeight_std_gt0')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z7ByMV9fCzJv",
        "outputId": "2e7ee908-55b0-46fe-fd35-8c95c7e6bc7e"
      },
      "outputs": [],
      "source": [
        "group_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iddZ8tHlxC3",
        "outputId": "ee8e840f-0a42-468f-a7d8-ad18506e5a2d"
      },
      "outputs": [],
      "source": [
        "null_counts = group_stats.select([\n",
        "    pl.col(col).is_null().sum().alias(col) for col in group_stats.columns\n",
        "])\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Dtt6DZl8S2"
      },
      "outputs": [],
      "source": [
        "group_stats = group_stats.with_columns([\n",
        "    pl.col(\"OrderWeight_std\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_min\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_mean_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_median_gt0\").fill_null(0),\n",
        "    pl.col(\"OrderWeight_std_gt0\").fill_null(0)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoAbm8UZETl6",
        "outputId": "1f23fcdf-ad9a-4a9f-a237-8c49aaf78f8e"
      },
      "outputs": [],
      "source": [
        "def apply_stats(df: pl.DataFrame, group_stats: pl.DataFrame) -> pl.DataFrame:\n",
        "    df = df.join(group_stats, on=['ProductForPlan10', 'MODEL_4', 'month', 'week'], how='left')\n",
        "    return df\n",
        "\n",
        "transaction = apply_stats(transaction, group_stats)\n",
        "print(transaction.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wugnTYa9GABu"
      },
      "source": [
        "### Holiday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wzsP7wabGCeC",
        "outputId": "6c198be7-1a3f-40b2-da60-6d5dd1cece63"
      },
      "outputs": [],
      "source": [
        "event_date #newyear, songkarn, thudjene, makabusha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le35TRkCUhIh"
      },
      "outputs": [],
      "source": [
        "def apply_event(df: pl.DataFrame, event_date: pl.DataFrame) -> pl.DataFrame:\n",
        "    # Select only the needed columns\n",
        "    event_date = (\n",
        "        event_date\n",
        "        .select([\n",
        "            pl.col(\"date\").cast(pl.Date).alias(\"OrderDate\"),\n",
        "            \"is_songkarn\",\n",
        "            \"is_makabusha\"\n",
        "        ])\n",
        "    )\n",
        "    df = df.join(event_date, on=\"OrderDate\", how=\"left\")\n",
        "    return df\n",
        "\n",
        "transaction = apply_event(transaction, event_date)\n",
        "\n",
        "#newyear, songkarn, thudjene, makabusha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXOzhpZXoZgs"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P55wm5RGwcKp",
        "outputId": "63536059-d6d9-4740-a848-7dd7f88e982f"
      },
      "outputs": [],
      "source": [
        "print(transaction.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "wsQGw2Q0xP53",
        "outputId": "625f9751-edeb-4ead-fd83-924b7d56a37a"
      },
      "outputs": [],
      "source": [
        "transaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDHeDVGpB11U"
      },
      "source": [
        "### Load_Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "C00r4VcXB52T",
        "outputId": "5f4348f6-c88e-4029-e14a-f2208ffde2f8"
      },
      "outputs": [],
      "source": [
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8UVo8SnCZH7"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.with_columns(\n",
        "    pl.col(\"OrderDate\").str.strptime(pl.Datetime, \"%Y-%m-%d\", strict=False).alias(\"OrderDate\")\n",
        ")\n",
        "\n",
        "test_df = test_df.with_columns([\n",
        "    pl.col(\"OrderDate\").dt.weekday().alias(\"day_of_week\"),  # 0 = Monday\n",
        "    pl.col(\"OrderDate\").dt.month().alias(\"month\"),\n",
        "    pl.when(((pl.col(\"OrderDate\").dt.day() - 1) // 7 + 1) > 4)\n",
        "      .then(4)\n",
        "      .otherwise((pl.col(\"OrderDate\").dt.day() - 1) // 7 + 1)\n",
        "      .alias(\"week\"),\n",
        "    pl.col(\"OrderDate\").dt.year().alias(\"year\")  \n",
        "])\n",
        "\n",
        "test_df = test_df.with_columns([\n",
        "    pl.col(\"day_of_week\").map_elements(lambda x: np.sin(2 * np.pi * x / 7)).alias(\"day_of_week_sin\"),\n",
        "    pl.col(\"day_of_week\").map_elements(lambda x: np.cos(2 * np.pi * x / 7)).alias(\"day_of_week_cos\")\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2vTMKrBfC_w"
      },
      "outputs": [],
      "source": [
        "def add_column_from_join(\n",
        "    left_df: pl.DataFrame,\n",
        "    right_df: pl.DataFrame,\n",
        "    join_keys: list[str],\n",
        "    target_column: str,\n",
        "    how: str = \"left\"\n",
        ") -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Join left_df with right_df on join_keys, adding only target_column from right_df.\n",
        "\n",
        "    Args:\n",
        "        left_df (pl.DataFrame): The main DataFrame.\n",
        "        right_df (pl.DataFrame): The DataFrame to join from.\n",
        "        join_keys (list[str]): Columns to join on (must exist in both).\n",
        "        target_column (str): Column name to add from right_df.\n",
        "        how (str, optional): Join type. Defaults to \"left\".\n",
        "\n",
        "    Returns:\n",
        "        pl.DataFrame: Resulting DataFrame with target_column added.\n",
        "    \"\"\"\n",
        "    right_trimmed = right_df.select(join_keys + [target_column]).unique(subset=join_keys)\n",
        "    return left_df.join(right_trimmed, on=join_keys, how=how)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5bP6_ahdAWP"
      },
      "outputs": [],
      "source": [
        "test_df = add_column_from_join(\n",
        "    left_df=test_df,\n",
        "    right_df=transaction,\n",
        "    join_keys=[\"ProductForPlan10\", \"ProductForPlan1\"],\n",
        "    target_column=\"ProductForPlan8\"\n",
        ")\n",
        "\n",
        "test_df = apply_stats0(test_df, group_stats0)\n",
        "test_df = apply_stats4(test_df, group_stats4)\n",
        "test_df = apply_stats(test_df, group_stats)\n",
        "test_df = apply_stats1(test_df, group_stats1)\n",
        "test_df = apply_stats2(test_df, group_stats2)\n",
        "test_df = apply_stats3(test_df, group_stats3)\n",
        "\n",
        "test_df = test_df.with_columns(pl.col(\"OrderDate\").cast(pl.Date))\n",
        "test_df = apply_event(test_df, event_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhIGdPGgCrzI",
        "outputId": "440aefdf-60fc-4bbc-cd57-17153f9760eb"
      },
      "outputs": [],
      "source": [
        "test_df = test_df.to_pandas()\n",
        "\n",
        "print(test_df.info())\n",
        "print(f\"ProductForPlan1 unique count: {test_df['ProductForPlan1'].nunique()}\")\n",
        "print(f\"CustomerBKey unique count: {test_df['CustomerBKey'].nunique()}\")\n",
        "print(f\"ProductForPlan10 unique count: {test_df['ProductForPlan10'].nunique()}\")\n",
        "print(f\"MODEL_4 unique count: {test_df['MODEL_4'].nunique()}\")\n",
        "print(test_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSs-dpbjkGQD"
      },
      "outputs": [],
      "source": [
        "# Get all columns except 'OrderWeight'\n",
        "cols_to_fill = test_df.columns.difference(['OrderWeight'])\n",
        "\n",
        "# Fill NaNs in those columns with 0\n",
        "test_df[cols_to_fill] = test_df[cols_to_fill].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = transaction.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_df.info())\n",
        "print(f\"ProductForPlan1 unique count: {train_df['ProductForPlan1'].nunique()}\")\n",
        "print(f\"ProductForPlan10 unique count: {train_df['ProductForPlan10'].nunique()}\")\n",
        "print(f\"MODEL_4 unique count: {train_df['MODEL_4'].nunique()}\")\n",
        "print(train_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_columns = ['ProductForPlan1', 'ProductBKey', 'MODEL_4', 'ProductForPlan8',\n",
        "    'is_songkarn', 'is_makabusha', 'CustomerBKey',\n",
        "    'day_of_week', 'month', 'week', 'ProductForPlan10']\n",
        "train_df[cat_columns] = train_df[cat_columns].astype('category')\n",
        "test_df[cat_columns] = test_df[cat_columns].astype('category')\n",
        "\n",
        "train_df = train_df.drop(\"year\", axis=1)\n",
        "test_df = test_df.drop(\"year\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcW0dnIywme3"
      },
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phwFTJyHqfcm",
        "outputId": "1edb52f2-5804-4468-9975-4e4111326b71"
      },
      "outputs": [],
      "source": [
        "print(train_df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LGBMbyRegion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiWTX7uX6phl"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb # Make sure you import LGBMClassifier too\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, log_loss, accuracy_score, mean_absolute_error # Import classification metrics\n",
        "from lightgbm import LGBMClassifier\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "def autotrain(df, test_df): # Removed unused 'feature' parameter\n",
        "    region = df['MODEL_4'].unique()\n",
        "    model_list = []\n",
        "    all_predictions = []\n",
        "    feature_importances_records = []\n",
        "    classifier_list = []\n",
        "\n",
        "    target = 'OrderWeight'\n",
        "    \n",
        "    base_features = ['CustomerBKey', 'ProductForPlan10', 'ProductForPlan1', 'ProductBKey',\n",
        "       'ProductForPlan8', 'day_of_week',\n",
        "       'month', 'week', 'OrderWeight_people_mean', 'OrderWeight_people_median',\n",
        "       'OrderWeight_people_std', 'OrderWeight_people_max',\n",
        "       'OrderWeight_people_min', 'OrderWeight_people_mean_gt0',\n",
        "       'OrderWeight_people_median_gt0', 'OrderWeight_people_std_gt0',\n",
        "       'OrderWeight_people_mean_inmonth', 'OrderWeight_people_median_inmonth',\n",
        "       'OrderWeight_people_std_inmonth', 'OrderWeight_people_cycle_mean', \n",
        "       'OrderWeight_people_cycle_median', 'OrderWeight_people_cycle_std', \n",
        "       'OrderWeight_people_cycle_max', 'OrderWeight_people_cycle_min', \n",
        "       'OrderWeight_people_cycle_mean_gt0', 'OrderWeight_people_cycle_median_gt0', \n",
        "       'OrderWeight_people_cycle_std_gt0', \n",
        "       'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max',\n",
        "       'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "       'OrderWeight_std_gt0', 'is_new_year', 'is_songkarn', 'is_thudjene',\n",
        "       'is_makabusha']\n",
        "    \n",
        "    categorical_features = [\n",
        "    'CustomerBKey', 'ProductForPlan10', 'day_of_week', 'month', 'week', \n",
        "    'ProductForPlan1', 'ProductBKey', 'ProductForPlan8',\n",
        "    'is_new_year', 'is_songkarn', 'is_thudjene', 'is_makabusha', 'CustomerBKey'\n",
        "    ]\n",
        "    # Add your engineered features here if you have them!\n",
        "\n",
        "    # --- Step 1: Create the binary target 'has_sales' ---\n",
        "    df['has_sales'] = (df[target] > 0).astype(int)\n",
        "    # test_df does not have 'units_sold', so 'has_sales' cannot be created directly.\n",
        "    # The classifier's prediction will provide this information for test_df.\n",
        "\n",
        "    # --- Define a fixed random state for reproducibility ---\n",
        "    FIXED_RANDOM_STATE = 42\n",
        "\n",
        "    for region_pred in region:\n",
        "        print(f\"\\nProcessing Region {region_pred}...\")\n",
        "        region_df = df[df['MODEL_4'] == region_pred].copy()\n",
        "        region_test_df = test_df[test_df['MODEL_4'] == region_pred].copy()\n",
        "\n",
        "        current_features = base_features # Use the defined base features\n",
        "\n",
        "        missing = [col for col in current_features if col not in region_df.columns]\n",
        "        if missing:\n",
        "            print(f\"Store {region_pred} missing features: {missing}\")\n",
        "            continue\n",
        "\n",
        "        X_region = region_df[current_features]\n",
        "        y_sales_region = region_df[target]\n",
        "        y_has_sales_region = region_df['has_sales'] # Use the binary target\n",
        "\n",
        "        # --- Step 2a: Train a Classifier to predict has_sales ---\n",
        "        print(f\"  Training classifier for region {region_pred}...\")\n",
        "        # Split ALL data for this store for classifier training and validation\n",
        "        X_train_clf, X_val_clf, y_train_has_sales, y_val_has_sales = train_test_split(\n",
        "            X_region, y_has_sales_region, test_size=0.2, shuffle=True, stratify=y_has_sales_region,\n",
        "            random_state=FIXED_RANDOM_STATE # <-- Added fixed random state\n",
        "        )\n",
        "\n",
        "        classifier = LGBMClassifier(\n",
        "            objective='binary',          # Binary classification objective\n",
        "            metric='logloss',            # Evaluation metric\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=63,               # Default LightGBM value, often good\n",
        "            verbose=-1,                  # Suppress verbose output\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1                    # Use all available cores\n",
        "        )\n",
        "\n",
        "        classifier.fit(\n",
        "            X_train_clf,\n",
        "            y_train_has_sales,\n",
        "            eval_set=[(X_val_clf, y_val_has_sales)],\n",
        "            eval_metric='logloss',\n",
        "            categorical_feature=categorical_features,\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)] # Add early stopping\n",
        "        )\n",
        "\n",
        "        val_probs = classifier.predict_proba(X_val_clf)[:, 1]\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_val_has_sales, val_probs)\n",
        "        optimal_threshold = thresholds[np.argmax(tpr - fpr)] + 0.02\n",
        "\n",
        "        print(f\"  Region {region_pred} Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "        # Predict probability of having sales (class 1) for ALL instances in store_df and store_test_df\n",
        "        # This probability will be used as a feature and for the final prediction logic\n",
        "        region_df['prob_has_sales'] = classifier.predict_proba(X_region)[:, 1]\n",
        "        region_test_df['prob_has_sales'] = classifier.predict_proba(region_test_df[current_features])[:, 1]\n",
        "\n",
        "        clf_logloss = log_loss(y_val_has_sales, classifier.predict_proba(X_val_clf)[:, 1])\n",
        "        print(f\"  Region {region_pred} Classifier Metrics | LogLoss: {clf_logloss:.4f}\") # Added accuracy optional above\n",
        "\n",
        "        # --- Step 2b: Train the Regressor (Option 1: on ALL data) ---\n",
        "        # --- Step 2b: Train the Regressor (LightGBM) ---\n",
        "        print(f\"  Training regressor for region {region_pred} on ALL data...\")\n",
        "\n",
        "        # Define features for the regressor (base features + classifier probability)\n",
        "        regressor_features = current_features + ['prob_has_sales']\n",
        "\n",
        "        # Split data for regressor training and validation\n",
        "        X_reg = region_df[regressor_features]\n",
        "        y_reg = region_df[target]\n",
        "\n",
        "        X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
        "            X_reg, y_reg, test_size=0.2, shuffle=True,\n",
        "            random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressor = LGBMRegressor(\n",
        "            objective='tweedie',\n",
        "            tweedie_variance_power=1.5,\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=8,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='mae',\n",
        "            verbose=-1\n",
        "        )\n",
        "\n",
        "        regressor.fit(\n",
        "            X_train_reg,\n",
        "            y_train_reg,\n",
        "            eval_set=[(X_val_reg, y_val_reg)],\n",
        "            categorical_feature=categorical_features,\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)]\n",
        "        )\n",
        "\n",
        "        print(\"  Regressor training complete.\")\n",
        "\n",
        "        # --- Get Feature Importances for the Regressor ---\n",
        "        importances = regressor.feature_importances_\n",
        "        for feature_name, importance_value in zip(regressor_features, importances):\n",
        "            feature_importances_records.append({\n",
        "                'store_id': region_pred,\n",
        "                'feature_name': feature_name,\n",
        "                'importance_value': importance_value\n",
        "            })\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': regressor_features,\n",
        "            'importance': importances\n",
        "        }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "        top5 = importance_df.head(5)\n",
        "        print(f\"  Top 5 important features for region {region_pred}:\")\n",
        "        print(top5.to_string(index=False))\n",
        "\n",
        "        val_reg_pred = regressor.predict(X_val_reg)\n",
        "\n",
        "        # Combine with classifier probability\n",
        "        final_val_pred = np.where(\n",
        "            X_val_reg['prob_has_sales'] > optimal_threshold,\n",
        "            val_reg_pred,\n",
        "            0\n",
        "        )\n",
        "        final_val_pred[final_val_pred < 0] = 0\n",
        "\n",
        "        mae_score = mean_absolute_error(y_val_reg, final_val_pred)\n",
        "\n",
        "        print(f\"  Region {region_pred} MAE: {mae_score:.4f}\")\n",
        "\n",
        "        # --- Predict on Test Data ---\n",
        "        if region_test_df.empty:\n",
        "            print(f\"  No test data for region {region_pred}.\")\n",
        "            continue\n",
        "\n",
        "        # store_test_df already has 'prob_has_sales' added earlier\n",
        "        X_test_combined = region_test_df[regressor_features] # Use the same regressor features\n",
        "\n",
        "        # Predict sales amount using the regressor on the test data\n",
        "        y_pred_test_reg = regressor.predict(X_test_combined)\n",
        "\n",
        "        # Apply the classification threshold logic to test predictions\n",
        "        y_pred_test_final = np.where(\n",
        "            X_test_combined['prob_has_sales'] > optimal_threshold, # Use the predicted probability from the classifier\n",
        "            y_pred_test_reg,\n",
        "            0 # Predict 0 if probability is below threshold\n",
        "        )\n",
        "\n",
        "        # Ensure final test predictions are non-negative\n",
        "        y_pred_test_final[y_pred_test_final < 0] = 0\n",
        "\n",
        "\n",
        "        region_test_df['predicted_sales'] = y_pred_test_final\n",
        "        all_predictions.append(region_test_df[['id', 'predicted_sales']])\n",
        "\n",
        "    # --- Consolidate predictions and merge back to test_df outside the loop ---\n",
        "    if all_predictions:\n",
        "      if 'predicted_sales' in test_df.columns:\n",
        "        test_df = test_df.drop(columns=['predicted_sales'])\n",
        "\n",
        "      prediction_df = pd.concat(all_predictions, ignore_index=True)\n",
        "\n",
        "      # Ensure 'id' exists and matches in test_df and prediction_df\n",
        "      test_df = test_df.merge(prediction_df, on='id', how='left')\n",
        "\n",
        "      # Fill missing predictions with 0\n",
        "      test_df['predicted_sales'] = test_df['predicted_sales'].fillna(0)\n",
        "\n",
        "\n",
        "    return model_list, test_df, feature_importances_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "_UYH0ChuFa_1",
        "outputId": "033d4e20-c8cb-4bb3-c695-98bb2f2aa2d4"
      },
      "outputs": [],
      "source": [
        "feature_importances_records = []\n",
        "model_list, test_df, feature_importances_records = autotrain(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoiFIfi4ctsH"
      },
      "source": [
        "#### LightGBMbyItem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bswIOrVNc2FP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, log_loss\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost.callback import EarlyStopping\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "def autotrain2(df, test_df):\n",
        "    items = df['ProductForPlan10'].unique()\n",
        "    model_list = []\n",
        "    all_predictions = []\n",
        "    feature_importances_records = []\n",
        "\n",
        "    target = 'OrderWeight'\n",
        "    base_features = ['CustomerBKey', 'ProductForPlan1', 'ProductBKey',\n",
        "       'ProductForPlan8', 'day_of_week', 'MODEL_4',\n",
        "       'month', 'week', 'OrderWeight_people_mean', 'OrderWeight_people_median',\n",
        "       'OrderWeight_people_std', 'OrderWeight_people_max',\n",
        "       'OrderWeight_people_min', 'OrderWeight_people_mean_gt0',\n",
        "       'OrderWeight_people_median_gt0', 'OrderWeight_people_std_gt0',\n",
        "       'OrderWeight_people_mean_inmonth', 'OrderWeight_people_median_inmonth',\n",
        "       'OrderWeight_people_std_inmonth', 'OrderWeight_mean',\n",
        "       'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max',\n",
        "       'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "       'OrderWeight_std_gt0', 'is_new_year', 'is_songkarn', 'is_thudjene',\n",
        "       'is_makabusha']\n",
        "    \n",
        "    categorical_features = [\n",
        "    'CustomerBKey', 'MODEL_4', 'day_of_week', 'month', 'week', \n",
        "    'ProductForPlan1', 'ProductBKey', 'ProductForPlan8',\n",
        "    'is_new_year', 'is_songkarn', 'is_thudjene', 'is_makabusha', 'CustomerBKey'\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Create binary target\n",
        "    df['has_sales'] = (df[target] > 0).astype(int)\n",
        "\n",
        "    FIXED_RANDOM_STATE = 42\n",
        "\n",
        "    for item in items:\n",
        "        print(f\"\\nProcessing Item {item}...\")\n",
        "        item_df = df[df['ProductForPlan10'] == item].copy()\n",
        "        item_test_df = test_df[test_df['ProductForPlan10'] == item].copy()\n",
        "\n",
        "        current_features = base_features\n",
        "\n",
        "        missing = [col for col in current_features if col not in item_df.columns]\n",
        "        if missing:\n",
        "            print(f\"Item {item} missing features: {missing}\")\n",
        "            continue\n",
        "\n",
        "        X_item = item_df[current_features]\n",
        "        y_sales_item = item_df[target]\n",
        "        y_has_sales_item = item_df['has_sales']\n",
        "\n",
        "        # Train classifier\n",
        "        print(f\"  Training classifier for item {item}...\")\n",
        "        X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
        "            X_item, y_has_sales_item, test_size=0.2, stratify=y_has_sales_item,\n",
        "            random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        classifier = LGBMClassifier(\n",
        "            objective='binary',\n",
        "            metric='logloss',\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=31,\n",
        "            verbose=-1,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        classifier.fit(\n",
        "            X_train_clf, y_train_clf,\n",
        "            eval_set=[(X_val_clf, y_val_clf)],\n",
        "            eval_metric='logloss',\n",
        "            categorical_feature=categorical_features,\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)]\n",
        "        )\n",
        "\n",
        "        val_probs = classifier.predict_proba(X_val_clf)[:, 1]\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_val_clf, val_probs)\n",
        "        optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "\n",
        "        print(f\"  Item {item} Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "        # Apply threshold to full item_df and test set\n",
        "        item_df['prob_has_sales'] = classifier.predict_proba(X_item)[:, 1]\n",
        "        item_test_df['prob_has_sales'] = classifier.predict_proba(item_test_df[current_features])[:, 1]\n",
        "\n",
        "\n",
        "        clf_logloss = log_loss(y_val_clf, classifier.predict_proba(X_val_clf)[:, 1])\n",
        "        print(f\"  Item {item} Classifier LogLoss: {clf_logloss:.4f}\")\n",
        "\n",
        "        # Train regressor\n",
        "        print(f\"  Training regressor for item {item}...\")\n",
        "        regressor_features = current_features + ['prob_has_sales']\n",
        "\n",
        "        X_reg = item_df[regressor_features]\n",
        "        y_reg = item_df[target]\n",
        "\n",
        "        X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
        "            X_reg, y_reg, test_size=0.2, random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressor = LGBMRegressor(\n",
        "            objective='tweedie',\n",
        "            tweedie_variance_power=1.5,\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=6,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='mae',\n",
        "            verbose=-1\n",
        "        )\n",
        "\n",
        "        regressor.fit(\n",
        "            X_train_reg,\n",
        "            y_train_reg,\n",
        "            eval_set=[(X_val_reg, y_val_reg)],\n",
        "            categorical_feature=categorical_features,\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)]\n",
        "        )\n",
        "\n",
        "        importances = regressor.feature_importances_\n",
        "        for feature_name, importance_value in zip(regressor_features, importances):\n",
        "            feature_importances_records.append({\n",
        "                'item_id': item,\n",
        "                'feature_name': feature_name,\n",
        "                'importance_value': importance_value\n",
        "            })\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': regressor_features,\n",
        "            'importance': importances\n",
        "        }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "        top5 = importance_df.head(5)\n",
        "        print(f\"  Top 5 important features for item {item}:\")\n",
        "        print(top5.to_string(index=False))\n",
        "\n",
        "        val_pred_reg = regressor.predict(X_val_reg)\n",
        "        final_val_pred = np.where(\n",
        "            X_val_reg['prob_has_sales'] > optimal_threshold,\n",
        "            val_pred_reg,\n",
        "            0\n",
        "        )\n",
        "        final_val_pred[final_val_pred < 0] = 0\n",
        "\n",
        "        mae_score = mean_absolute_error(y_val_reg, final_val_pred)\n",
        "        print(f\"  Item {item} MAE: {mae_score:.4f}\")\n",
        "\n",
        "        # Predict on test set\n",
        "        if item_test_df.empty:\n",
        "            print(f\"  No test data for item {item}.\")\n",
        "            continue\n",
        "\n",
        "        X_test_combined = item_test_df[regressor_features]\n",
        "        y_pred_test_reg = regressor.predict(X_test_combined)\n",
        "        y_pred_test_final = np.where(\n",
        "            X_test_combined['prob_has_sales'] > optimal_threshold,\n",
        "            y_pred_test_reg,\n",
        "            0\n",
        "        )\n",
        "        y_pred_test_final[y_pred_test_final < 0] = 0\n",
        "\n",
        "        item_test_df['predicted_sales'] = y_pred_test_final\n",
        "        all_predictions.append(item_test_df[['id', 'predicted_sales']])\n",
        "\n",
        "    # Combine predictions\n",
        "    if all_predictions:\n",
        "        if 'predicted_sales' in test_df.columns:\n",
        "            test_df = test_df.drop(columns=['predicted_sales'])\n",
        "        prediction_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        test_df = test_df.merge(prediction_df, on='id', how='left')\n",
        "        test_df['predicted_sales'] = test_df['predicted_sales'].fillna(0)\n",
        "\n",
        "    return model_list, test_df, feature_importances_records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufXILPgOesHN",
        "outputId": "d8c17843-91f9-42b3-9705-9443c5629a40"
      },
      "outputs": [],
      "source": [
        "feature_importances_records = []\n",
        "model_list, test_df, feature_importances_records = autotrain2(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, log_loss\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "def autotrain3(df, test_df):\n",
        "    items = df['ProductForPlan10'].unique()\n",
        "    model_list = []\n",
        "    all_predictions = []\n",
        "    feature_importances_records = []\n",
        "\n",
        "    target = 'OrderWeight'\n",
        "    base_features = ['CustomerBKey', 'ProductForPlan1', 'ProductBKey', \n",
        "                     'MODEL_4', 'ProductForPlan8', 'day_of_week', \n",
        "                     'month', 'week', 'OrderWeight_people_mean', 'OrderWeight_people_median', \n",
        "                     'OrderWeight_people_std', 'OrderWeight_people_max', 'OrderWeight_people_min', \n",
        "                     'OrderWeight_people_last_month_mean', 'OrderWeight_people_last_month_std', \n",
        "                     'OrderWeight_people_last_month_max', 'OrderWeight_people_last_month_min', \n",
        "                     'OrderWeight_people_last_month_mean_gt0', 'OrderWeight_people_last_month_median_gt0', \n",
        "                     'OrderWeight_people_last_month_std_gt0',\n",
        "                     'OrderWeight_people_mean_gt0', 'OrderWeight_people_median_gt0', \n",
        "                     'OrderWeight_people_std_gt0', 'OrderWeight_people_mean_inmonth', \n",
        "                     'OrderWeight_people_median_inmonth', 'OrderWeight_people_std_inmonth', \n",
        "                     'OrderWeight_people_cycle_mean', 'OrderWeight_people_cycle_median', \n",
        "                     'OrderWeight_people_cycle_std', 'OrderWeight_people_cycle_max', \n",
        "                     'OrderWeight_people_cycle_min', 'OrderWeight_people_cycle_mean_gt0', \n",
        "                     'OrderWeight_people_cycle_median_gt0', 'OrderWeight_people_cycle_std_gt0', \n",
        "                     'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max', \n",
        "                     'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "                     'OrderWeight_people_last_month_week_mean',          \n",
        "                     'OrderWeight_people_last_month_week_std',           \n",
        "                     'OrderWeight_people_last_month_week_max',\n",
        "                     'OrderWeight_people_last_month_week_min',           \n",
        "                     'OrderWeight_people_last_month_week_mean_gt0',\n",
        "                     \"OrderWeight_people_last_month_week_median_gt0\",\n",
        "                     'OrderWeight_people_last_month_week_std_gt0',\n",
        "                     'OrderWeight_std_gt0', 'is_songkarn', 'is_makabusha',\n",
        "                     \"day_of_week_sin\", \"day_of_week_cos\"]\n",
        "    \n",
        "    categorical_features = [\n",
        "    'ProductForPlan1', 'ProductBKey', 'MODEL_4', 'ProductForPlan8',\n",
        "    'is_songkarn', 'is_makabusha', 'CustomerBKey',\n",
        "    'day_of_week', 'month', 'week',\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Create binary target\n",
        "    df['has_sales'] = (df[target] > 0).astype(int)\n",
        "\n",
        "    FIXED_RANDOM_STATE = 42\n",
        "\n",
        "    for item in items:\n",
        "        print(f\"\\nProcessing Item {item}...\")\n",
        "        item_df = df[df['ProductForPlan10'] == item].copy()\n",
        "        item_test_df = test_df[test_df['ProductForPlan10'] == item].copy()\n",
        "\n",
        "        current_features = base_features\n",
        "\n",
        "        missing = [col for col in current_features if col not in item_df.columns]\n",
        "        if missing:\n",
        "            print(f\"Item {item} missing features: {missing}\")\n",
        "            continue\n",
        "\n",
        "        X_item = item_df[current_features]\n",
        "        y_sales_item = item_df[target]\n",
        "        y_has_sales_item = item_df['has_sales']\n",
        "\n",
        "        # Train classifier\n",
        "        print(f\"  Training classifier for item {item}...\")\n",
        "        X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
        "            X_item, y_has_sales_item, test_size=0.2, stratify=y_has_sales_item,\n",
        "            random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        classifier = LGBMClassifier(\n",
        "            objective='binary',\n",
        "            metric='binary_logloss',\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=64,\n",
        "            max_depth=10,\n",
        "            verbose=-1,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        classifier.fit(\n",
        "            X_train_clf, y_train_clf,\n",
        "            eval_set=[(X_val_clf, y_val_clf)],\n",
        "            eval_metric='logloss',\n",
        "            categorical_feature=categorical_features,\n",
        "            callbacks=[lgb.early_stopping(10, verbose=False)]\n",
        "        )\n",
        "\n",
        "        val_probs = classifier.predict_proba(X_val_clf)[:, 1]\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_val_clf, val_probs)\n",
        "        optimal_threshold = thresholds[np.argmax(tpr - fpr)] + 0.05\n",
        "\n",
        "        print(f\"  Item {item} Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "        # Apply threshold to full item_df and test set\n",
        "        item_df['prob_has_sales'] = classifier.predict_proba(X_item)[:, 1]\n",
        "        item_test_df['prob_has_sales'] = classifier.predict_proba(item_test_df[current_features])[:, 1]\n",
        "\n",
        "\n",
        "        clf_logloss = log_loss(y_val_clf, classifier.predict_proba(X_val_clf)[:, 1])\n",
        "        print(f\"  Item {item} Classifier LogLoss: {clf_logloss:.4f}\")\n",
        "\n",
        "        # Train regressor\n",
        "        print(f\"  Training regressor for item {item}...\")\n",
        "        regressor_features = current_features + ['prob_has_sales']\n",
        "\n",
        "        X_reg = item_df[regressor_features]\n",
        "        y_reg = item_df[target]\n",
        "\n",
        "        X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
        "            X_reg, y_reg, test_size=0.2, random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressor = XGBRegressor(\n",
        "            objective='reg:tweedie',\n",
        "            tweedie_variance_power=1.5,\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=12,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='mae',\n",
        "            verbosity=0,\n",
        "            enable_categorical=True\n",
        "        )\n",
        "\n",
        "        # Removed: early_stopping_callback_xgb definition\n",
        "        regressor.fit(\n",
        "            X_train_reg,\n",
        "            y_train_reg,\n",
        "            # Removed: eval_set and callbacks arguments from regressor.fit()\n",
        "        )\n",
        "\n",
        "        importances = regressor.feature_importances_\n",
        "        for feature_name, importance_value in zip(regressor_features, importances):\n",
        "            feature_importances_records.append({\n",
        "                'item_id': item,\n",
        "                'feature_name': feature_name,\n",
        "                'importance_value': importance_value\n",
        "            })\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': regressor_features,\n",
        "            'importance': importances\n",
        "        }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "        top5 = importance_df.head(5)\n",
        "        print(f\"  Top 5 important features for item {item}:\")\n",
        "        print(top5.to_string(index=False))\n",
        "\n",
        "        val_pred_reg = regressor.predict(X_val_reg)\n",
        "        final_val_pred = np.where(\n",
        "            X_val_reg['prob_has_sales'] > optimal_threshold,\n",
        "            val_pred_reg,\n",
        "            0\n",
        "        )\n",
        "        final_val_pred[final_val_pred < 0] = 0\n",
        "\n",
        "        mae_score = mean_absolute_error(y_val_reg, final_val_pred)\n",
        "        print(f\"  Item {item} MAE: {mae_score:.4f}\")\n",
        "\n",
        "        # Predict on test set\n",
        "        if item_test_df.empty:\n",
        "            print(f\"  No test data for item {item}.\")\n",
        "            continue\n",
        "\n",
        "        X_test_combined = item_test_df[regressor_features]\n",
        "        y_pred_test_reg = regressor.predict(X_test_combined)\n",
        "        y_pred_test_final = np.where(\n",
        "            X_test_combined['prob_has_sales'] > optimal_threshold,\n",
        "            y_pred_test_reg,\n",
        "            0\n",
        "        )\n",
        "        y_pred_test_final[y_pred_test_final < 0] = 0\n",
        "\n",
        "        item_test_df['predicted_sales'] = y_pred_test_final\n",
        "        all_predictions.append(item_test_df[['id', 'predicted_sales']])\n",
        "\n",
        "    # Combine predictions\n",
        "    if all_predictions:\n",
        "        prediction_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        test_df = test_df.merge(prediction_df, on='id', how='left')\n",
        "        test_df['predicted_sales'] = test_df['predicted_sales'].fillna(0)\n",
        "\n",
        "    return model_list, test_df, feature_importances_records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_records = []\n",
        "model_list, test_df, feature_importances_records = autotrain3(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autogluon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = train_df.drop(columns=['day_of_week', 'week', 'month'])\n",
        "test_df = test_df.drop(columns=['day_of_week', 'week', 'month'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = TabularPredictor(label='OrderWeight').fit(\n",
        "    train_data=train_df,\n",
        "    num_gpus=1,\n",
        "    presets='good'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = predictor.predict(test_df.drop(columns=['OrderWeight']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['OrderWeight'] = y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop day_of_week and month\n",
        "test_df = test_df.drop(columns=['ProductForPlan8',\n",
        "                                'OrderWeight_people_mean', 'OrderWeight_people_median', \n",
        "                     'OrderWeight_people_std', 'OrderWeight_people_max', 'OrderWeight_people_min', \n",
        "                     'OrderWeight_people_last_month_mean', 'OrderWeight_people_last_month_std', \n",
        "                     'OrderWeight_people_last_month_max', 'OrderWeight_people_last_month_min', \n",
        "                     'OrderWeight_people_last_month_mean_gt0', 'OrderWeight_people_last_month_median_gt0', \n",
        "                     'OrderWeight_people_last_month_std_gt0',\n",
        "                     'OrderWeight_people_mean_gt0', 'OrderWeight_people_median_gt0', \n",
        "                     'OrderWeight_people_std_gt0', 'OrderWeight_people_mean_inmonth', \n",
        "                     'OrderWeight_people_median_inmonth', 'OrderWeight_people_std_inmonth', \n",
        "                     'OrderWeight_people_cycle_mean', 'OrderWeight_people_cycle_median', \n",
        "                     'OrderWeight_people_cycle_std', 'OrderWeight_people_cycle_max', \n",
        "                     'OrderWeight_people_cycle_min', 'OrderWeight_people_cycle_mean_gt0', \n",
        "                     'OrderWeight_people_cycle_median_gt0', 'OrderWeight_people_cycle_std_gt0', \n",
        "                     'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max', \n",
        "                     'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "                     'OrderWeight_people_last_month_week_mean',          \n",
        "                     'OrderWeight_people_last_month_week_std',           \n",
        "                     'OrderWeight_people_last_month_week_max',\n",
        "                     'OrderWeight_people_last_month_week_min',           \n",
        "                     'OrderWeight_people_last_month_week_mean_gt0',\n",
        "                     \"OrderWeight_people_last_month_week_median_gt0\",\n",
        "                     'OrderWeight_people_last_month_week_std_gt0',\n",
        "                     'OrderWeight_std_gt0', 'is_songkarn', 'is_makabusha',\n",
        "                     \"day_of_week_sin\", \"day_of_week_cos\"])\n",
        "\n",
        "# Optional: preview result\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-inflated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, log_loss\n",
        "from sklego.meta import ZeroInflatedRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "def autotrain4(df, test_df): # Removed unused 'feature' parameter\n",
        "    regions = df['MODEL_4'].unique()\n",
        "    model_list = []\n",
        "    all_predictions = []\n",
        "    feature_importances_records = []\n",
        "    classifier_list = []\n",
        "\n",
        "    target = 'OrderWeight'\n",
        "    \n",
        "    base_features = ['CustomerBKey', 'ProductForPlan10', 'ProductForPlan1', 'ProductBKey',\n",
        "       'ProductForPlan8', 'day_of_week',\n",
        "       'month', 'week', 'OrderWeight_people_mean', 'OrderWeight_people_median',\n",
        "       'OrderWeight_people_std', 'OrderWeight_people_max',\n",
        "       'OrderWeight_people_min', 'OrderWeight_people_mean_gt0',\n",
        "       'OrderWeight_people_median_gt0', 'OrderWeight_people_std_gt0',\n",
        "       'OrderWeight_people_mean_inmonth', 'OrderWeight_people_median_inmonth',\n",
        "       'OrderWeight_people_std_inmonth', 'OrderWeight_people_cycle_mean', \n",
        "       'OrderWeight_people_cycle_median', 'OrderWeight_people_cycle_std', \n",
        "       'OrderWeight_people_cycle_max', 'OrderWeight_people_cycle_min', \n",
        "       'OrderWeight_people_cycle_mean_gt0', 'OrderWeight_people_cycle_median_gt0', \n",
        "       'OrderWeight_people_cycle_std_gt0', \n",
        "       'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max',\n",
        "       'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "       'OrderWeight_std_gt0', 'is_new_year', 'is_songkarn', 'is_thudjene',\n",
        "       'is_makabusha']\n",
        "    \n",
        "    categorical_features = [\n",
        "    'CustomerBKey', 'ProductForPlan10', 'day_of_week', 'month', 'week', \n",
        "    'ProductForPlan1', 'ProductBKey', 'ProductForPlan8',\n",
        "    'is_new_year', 'is_songkarn', 'is_thudjene', 'is_makabusha', 'CustomerBKey'\n",
        "    ]\n",
        "\n",
        "    # --- Step 1: Create the binary target 'has_sales' ---\n",
        "    df['has_sales'] = (df[target] > 0).astype(int)\n",
        "    # test_df does not have 'units_sold', so 'has_sales' cannot be created directly.\n",
        "    # The classifier's prediction will provide this information for test_df.\n",
        "\n",
        "    # --- Define a fixed random state for reproducibility ---\n",
        "    FIXED_RANDOM_STATE = 42\n",
        "\n",
        "    for region_pred in regions:\n",
        "        print(f\"\\nProcessing Region {region_pred}...\")\n",
        "\n",
        "        # Filter training data for the current region\n",
        "        region_mask = train_df['MODEL_4'] == region_pred\n",
        "        region_df = train_df[region_mask].copy()\n",
        "\n",
        "        # Filter test data for the current region\n",
        "        test_region_mask = test_df['MODEL_4'] == region_pred\n",
        "        region_test_df = test_df[test_region_mask].copy()\n",
        "\n",
        "        # Skip if no training data for the current region\n",
        "        if region_df.empty:\n",
        "            print(f\"No training data for region {region_pred}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Define X and y for the current region's training data\n",
        "        X_region = region_df[base_features]\n",
        "        y_region = region_df[target]\n",
        "\n",
        "        # Split region data into training and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_region, y_region,\n",
        "            test_size=0.2,\n",
        "            random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        # Initialize LGBMClassifier for the zero-inflated part (predicting presence of sales)\n",
        "        classifier = LGBMClassifier(\n",
        "            objective='binary',\n",
        "            metric='logloss',\n",
        "            n_estimators=150,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=31,\n",
        "            verbose=-1, # Suppress verbose output\n",
        "            categorical_feature=categorical_features,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1 # Use all available CPU cores\n",
        "        )\n",
        "\n",
        "        # Initialize LGBMRegressor for the regression part (predicting sales amount when present)\n",
        "        # 'tweedie' objective is suitable for target variables with many zeros and positive values\n",
        "        regressor = LGBMRegressor(\n",
        "            objective='tweedie',\n",
        "            tweedie_variance_power=1.5, # Power parameter for Tweedie distribution\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=6,\n",
        "            categorical_feature=categorical_features,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            verbose=-1,\n",
        "            boosting_type='goss'\n",
        "        )\n",
        "\n",
        "        # Combine classifier and regressor into a ZeroInflatedRegressor\n",
        "        zir_model = ZeroInflatedRegressor(\n",
        "            classifier=classifier,\n",
        "            regressor=regressor\n",
        "        )\n",
        "\n",
        "        # Train the Zero Inflated Regressor model\n",
        "        zir_model.fit(X_train, y_train)\n",
        "        model_list.append((region_pred, zir_model))\n",
        "\n",
        "        # --- Validation on the current region's validation set ---\n",
        "        # Predict probability of having sales using the classifier\n",
        "        prob_has_sales = zir_model.classifier_.predict_proba(X_val)[:, 1]\n",
        "        # Predict sales amount using the regressor\n",
        "        y_pred_reg = zir_model.regressor_.predict(X_val)\n",
        "        # Combine predictions: if probability of sales > 0.6, use regressor's prediction, else 0\n",
        "        y_pred_final = np.where(prob_has_sales > 0.35, y_pred_reg, 0)\n",
        "        # Clip predictions to ensure non-negative values\n",
        "        y_pred_final = np.clip(y_pred_final, a_min=0, a_max=None)\n",
        "\n",
        "        # Calculate Mean Absolute Error (MAE) for validation\n",
        "        mae_score = mean_absolute_error(y_val, y_pred_final)\n",
        "        print(f\"  Region {region_pred} MAE: {mae_score:.4f}\")\n",
        "\n",
        "        # --- Feature Importance ---\n",
        "        # Get feature importances from the regressor model\n",
        "        importances = zir_model.regressor_.feature_importances_\n",
        "        for feature, importance in zip(base_features, importances):\n",
        "            feature_importances_records.append({\n",
        "                'region': region_pred,\n",
        "                'feature': feature,\n",
        "                'importance': importance\n",
        "            })\n",
        "\n",
        "        # --- Test Predictions for the current region ---\n",
        "        if not region_test_df.empty:\n",
        "            X_test = region_test_df[base_features]\n",
        "            # Predict probability of sales for test data\n",
        "            prob_test = zir_model.classifier_.predict_proba(X_test)[:, 1]\n",
        "            # Predict sales amount for test data\n",
        "            y_pred_test_reg = zir_model.regressor_.predict(X_test)\n",
        "            # Combine test predictions\n",
        "            y_pred_test_final = np.where(prob_test > 0.6, y_pred_test_reg, 0)\n",
        "            # Clip test predictions\n",
        "            y_pred_test_final = np.clip(y_pred_test_final, a_min=0, a_max=None)\n",
        "\n",
        "            # Add predictions to the region's test DataFrame\n",
        "            region_test_df['predicted_OrderWeight'] = y_pred_test_final\n",
        "            # Append the 'id' and 'predicted_OrderWeight' columns to all_predictions list\n",
        "            all_predictions.append(region_test_df[['id', 'predicted_OrderWeight']])\n",
        "\n",
        "    # --- Combine all region predictions into a final test DataFrame ---\n",
        "    if all_predictions:\n",
        "        # Concatenate all regional predictions\n",
        "        prediction_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        # Merge predictions back to the original test_df based on 'id'\n",
        "        final_test_df = test_df.merge(\n",
        "            prediction_df,\n",
        "            on='id',\n",
        "            how='left'\n",
        "        )\n",
        "    else:\n",
        "        # If no predictions were made (e.g., all regions had no training data),\n",
        "        # create a copy of the original test_df and initialize 'predicted_OrderWeight' to 0\n",
        "        final_test_df = test_df.copy()\n",
        "        final_test_df['predicted_OrderWeight'] = 0\n",
        "\n",
        "    # Fill any remaining missing predictions (e.g., for regions not processed) with 0\n",
        "    final_test_df['predicted_OrderWeight'] = final_test_df['predicted_OrderWeight'].fillna(0)\n",
        "\n",
        "    return model_list, final_test_df, feature_importances_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_records = []\n",
        "model_list, test_df, feature_importances_records = autotrain4(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRAZY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, log_loss\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "def autotrain5(df, test_df):\n",
        "    people = df['CustomerBKey'].unique()\n",
        "    model_list = []\n",
        "    all_predictions = []\n",
        "    feature_importances_records = []\n",
        "\n",
        "    target = 'OrderWeight'\n",
        "    base_features = ['ProductForPlan10', 'ProductForPlan1', 'ProductBKey', \n",
        "                     'MODEL_4', 'ProductForPlan8', 'day_of_week', \n",
        "                     'month', 'week', 'OrderWeight_people_mean', 'OrderWeight_people_median', \n",
        "                     'OrderWeight_people_std', 'OrderWeight_people_max', 'OrderWeight_people_min', \n",
        "                     'OrderWeight_people_last_month_mean', 'OrderWeight_people_last_month_std', \n",
        "                     'OrderWeight_people_last_month_max', 'OrderWeight_people_last_month_min', \n",
        "                     'OrderWeight_people_last_month_mean_gt0', 'OrderWeight_people_last_month_median_gt0', \n",
        "                     'OrderWeight_people_last_month_std_gt0',\n",
        "                     'OrderWeight_people_mean_gt0', 'OrderWeight_people_median_gt0', \n",
        "                     'OrderWeight_people_std_gt0', 'OrderWeight_people_mean_inmonth', \n",
        "                     'OrderWeight_people_median_inmonth', 'OrderWeight_people_std_inmonth', \n",
        "                     'OrderWeight_people_cycle_mean', 'OrderWeight_people_cycle_median', \n",
        "                     'OrderWeight_people_cycle_std', 'OrderWeight_people_cycle_max', \n",
        "                     'OrderWeight_people_cycle_min', 'OrderWeight_people_cycle_mean_gt0', \n",
        "                     'OrderWeight_people_cycle_median_gt0', 'OrderWeight_people_cycle_std_gt0', \n",
        "                     'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max', \n",
        "                     'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "                     'OrderWeight_people_last_month_week_mean',          \n",
        "                     'OrderWeight_people_last_month_week_std',           \n",
        "                     'OrderWeight_people_last_month_week_max',\n",
        "                     'OrderWeight_people_last_month_week_min',           \n",
        "                     'OrderWeight_people_last_month_week_mean_gt0',\n",
        "                     \"OrderWeight_people_last_month_week_median_gt0\",\n",
        "                     'OrderWeight_people_last_month_week_std_gt0',\n",
        "                     'OrderWeight_std_gt0', 'is_songkarn', 'is_makabusha',\n",
        "                     \"day_of_week_sin\", \"day_of_week_cos\"]\n",
        "    \n",
        "    categorical_features = [\n",
        "    'ProductForPlan1', 'ProductBKey', 'MODEL_4', 'ProductForPlan8',\n",
        "    'is_songkarn', 'is_makabusha', 'ProductForPlan10',\n",
        "    'day_of_week', 'month', 'week',\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Create binary target\n",
        "    df['has_sales'] = (df[target] > 0).astype(int)\n",
        "\n",
        "    FIXED_RANDOM_STATE = 42\n",
        "\n",
        "    for item in people:\n",
        "        print(f\"\\nProcessing Item {item}...\")\n",
        "        item_df = df[df['CustomerBKey'] == item].copy()\n",
        "        item_test_df = test_df[test_df['CustomerBKey'] == item].copy()\n",
        "\n",
        "        current_features = base_features\n",
        "\n",
        "        missing = [col for col in current_features if col not in item_df.columns]\n",
        "        if missing:\n",
        "            print(f\"Item {item} missing features: {missing}\")\n",
        "            continue\n",
        "\n",
        "        X_item = item_df[current_features]\n",
        "        y_sales_item = item_df[target]\n",
        "        y_has_sales_item = item_df['has_sales']\n",
        "\n",
        "        # Train classifier\n",
        "        print(f\"  Training classifier for item {item}...\")\n",
        "        X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
        "            X_item, y_has_sales_item, test_size=0.2, stratify=y_has_sales_item,\n",
        "            random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        classifier = XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=10,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss',\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            enable_categorical=True  # only available if using recent xgboost & categorical inputs encoded correctly\n",
        "        )\n",
        "\n",
        "        classifier.fit(\n",
        "            X_train_clf,\n",
        "            y_train_clf,\n",
        "            eval_set=[(X_val_clf, y_val_clf)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        val_probs = classifier.predict_proba(X_val_clf)[:, 1]\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(y_val_clf, val_probs)\n",
        "        optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
        "\n",
        "        print(f\"  Item {item} Optimal Threshold: {optimal_threshold:.4f}\")\n",
        "\n",
        "        # Apply threshold to full item_df and test set\n",
        "        item_df['prob_has_sales'] = classifier.predict_proba(X_item)[:, 1]\n",
        "        item_test_df['prob_has_sales'] = classifier.predict_proba(item_test_df[current_features])[:, 1]\n",
        "\n",
        "\n",
        "        clf_logloss = log_loss(y_val_clf, classifier.predict_proba(X_val_clf)[:, 1])\n",
        "        print(f\"  Item {item} Classifier LogLoss: {clf_logloss:.4f}\")\n",
        "\n",
        "        # Train regressor\n",
        "        print(f\"  Training regressor for item {item}...\")\n",
        "        regressor_features = current_features + ['prob_has_sales']\n",
        "\n",
        "        X_reg = item_df[regressor_features]\n",
        "        y_reg = item_df[target]\n",
        "\n",
        "        X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
        "            X_reg, y_reg, test_size=0.2, random_state=FIXED_RANDOM_STATE\n",
        "        )\n",
        "\n",
        "        regressor = XGBRegressor(\n",
        "            objective='reg:tweedie',\n",
        "            tweedie_variance_power=1.5,\n",
        "            n_estimators=400,\n",
        "            learning_rate=0.01,\n",
        "            max_depth=10,\n",
        "            random_state=FIXED_RANDOM_STATE,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='mae',\n",
        "            verbosity=0,\n",
        "            enable_categorical=True\n",
        "        )\n",
        "\n",
        "        # Removed: early_stopping_callback_xgb definition\n",
        "        regressor.fit(\n",
        "            X_train_reg,\n",
        "            y_train_reg,\n",
        "            # Removed: eval_set and callbacks arguments from regressor.fit()\n",
        "        )\n",
        "\n",
        "        importances = regressor.feature_importances_\n",
        "        for feature_name, importance_value in zip(regressor_features, importances):\n",
        "            feature_importances_records.append({\n",
        "                'item_id': item,\n",
        "                'feature_name': feature_name,\n",
        "                'importance_value': importance_value\n",
        "            })\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': regressor_features,\n",
        "            'importance': importances\n",
        "        }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "        top5 = importance_df.head(5)\n",
        "        print(f\"  Top 5 important features for item {item}:\")\n",
        "        print(top5.to_string(index=False))\n",
        "\n",
        "        val_pred_reg = regressor.predict(X_val_reg)\n",
        "        final_val_pred = np.where(\n",
        "            X_val_reg['prob_has_sales'] > optimal_threshold,\n",
        "            val_pred_reg,\n",
        "            0\n",
        "        )\n",
        "        final_val_pred[final_val_pred < 0] = 0\n",
        "\n",
        "        mae_score = mean_absolute_error(y_val_reg, final_val_pred)\n",
        "        print(f\"  Item {item} MAE: {mae_score:.4f}\")\n",
        "\n",
        "        # Predict on test set\n",
        "        if item_test_df.empty:\n",
        "            print(f\"  No test data for item {item}.\")\n",
        "            continue\n",
        "\n",
        "        X_test_combined = item_test_df[regressor_features]\n",
        "        y_pred_test_reg = regressor.predict(X_test_combined)\n",
        "        y_pred_test_final = np.where(\n",
        "            X_test_combined['prob_has_sales'] > optimal_threshold,\n",
        "            y_pred_test_reg,\n",
        "            0\n",
        "        )\n",
        "        y_pred_test_final[y_pred_test_final < 0] = 0\n",
        "\n",
        "        item_test_df['predicted_sales'] = y_pred_test_final\n",
        "        all_predictions.append(item_test_df[['id', 'predicted_sales']])\n",
        "\n",
        "    # Combine predictions\n",
        "    if all_predictions:\n",
        "        prediction_df = pd.concat(all_predictions, ignore_index=True)\n",
        "        test_df = test_df.merge(prediction_df, on='id', how='left')\n",
        "        test_df['predicted_sales'] = test_df['predicted_sales'].fillna(0)\n",
        "\n",
        "    return model_list, test_df, feature_importances_records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importances_records = []\n",
        "model_list, test_df, feature_importances_records = autotrain5(train_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgyX4BLKPt2X"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace OrderWeight with predicted_sales\n",
        "test_df['OrderWeight'] = test_df['predicted_sales']\n",
        "\n",
        "# Drop day_of_week and month\n",
        "test_df = test_df.drop(columns=['day_of_week', 'month', 'predicted_sales', 'week', 'ProductForPlan8',\n",
        "                                'OrderWeight_people_mean', 'OrderWeight_people_median', \n",
        "                     'OrderWeight_people_std', 'OrderWeight_people_max', 'OrderWeight_people_min', \n",
        "                     'OrderWeight_people_last_month_mean', 'OrderWeight_people_last_month_std', \n",
        "                     'OrderWeight_people_last_month_max', 'OrderWeight_people_last_month_min', \n",
        "                     'OrderWeight_people_last_month_mean_gt0', 'OrderWeight_people_last_month_median_gt0', \n",
        "                     'OrderWeight_people_last_month_std_gt0',\n",
        "                     'OrderWeight_people_mean_gt0', 'OrderWeight_people_median_gt0', \n",
        "                     'OrderWeight_people_std_gt0', 'OrderWeight_people_mean_inmonth', \n",
        "                     'OrderWeight_people_median_inmonth', 'OrderWeight_people_std_inmonth', \n",
        "                     'OrderWeight_people_cycle_mean', 'OrderWeight_people_cycle_median', \n",
        "                     'OrderWeight_people_cycle_std', 'OrderWeight_people_cycle_max', \n",
        "                     'OrderWeight_people_cycle_min', 'OrderWeight_people_cycle_mean_gt0', \n",
        "                     'OrderWeight_people_cycle_median_gt0', 'OrderWeight_people_cycle_std_gt0', \n",
        "                     'OrderWeight_mean', 'OrderWeight_median', 'OrderWeight_std', 'OrderWeight_max', \n",
        "                     'OrderWeight_min', 'OrderWeight_mean_gt0', 'OrderWeight_median_gt0',\n",
        "                     'OrderWeight_people_last_month_week_mean',          \n",
        "                     'OrderWeight_people_last_month_week_std',           \n",
        "                     'OrderWeight_people_last_month_week_max',\n",
        "                     'OrderWeight_people_last_month_week_min',           \n",
        "                     'OrderWeight_people_last_month_week_mean_gt0',\n",
        "                     \"OrderWeight_people_last_month_week_median_gt0\",\n",
        "                     'OrderWeight_people_last_month_week_std_gt0',\n",
        "                     'OrderWeight_std_gt0', 'is_songkarn', 'is_makabusha',\n",
        "                     \"day_of_week_sin\", \"day_of_week_cos\"])\n",
        "\n",
        "# Optional: preview result\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "2bCajYI-QO4s",
        "outputId": "e4349fc9-e4df-4003-de4f-bdaa2554b600"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "VEXWAr0GX5tR",
        "outputId": "9dc3ddf5-97ae-46ab-8bf7-32fb3358611a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "unique_test_pairs = test_df[['CustomerBKey', 'ProductBKey']].drop_duplicates().values.tolist()\n",
        "\n",
        "if unique_test_pairs:\n",
        "    random_pair = random.choice(unique_test_pairs)\n",
        "    random_store_id, random_item_id = random_pair\n",
        "\n",
        "    print(f\"\\nRandomly selected pair: CustomerID = {random_store_id}, Item ID = {random_item_id}\")\n",
        "\n",
        "    pair_df = test_df[\n",
        "        (test_df['CustomerBKey'] == random_store_id) &\n",
        "        (test_df['ProductBKey'] == random_item_id)\n",
        "    ].copy()\n",
        "\n",
        "    item = pair_df[\"ProductForPlan10\"].iloc[0]\n",
        "    pair_df_21_days = pair_df.head(21)\n",
        "\n",
        "    # --- Plot using Plotly ---\n",
        "    fig = px.line(\n",
        "        pair_df_21_days,\n",
        "        x='OrderDate',\n",
        "        y='OrderWeight',\n",
        "        title=f'Store ID: {random_store_id}, Item ID: {random_item_id} - Predicted Sales (21 Days), item: {item}',\n",
        "        labels={'OrderDate': 'Date', 'OrderWeight': 'Predicted Sales'}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Predicted Sales',\n",
        "        xaxis_tickangle=-45,\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo unique store-item pairs found in the test data to plot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ip6fQh9x-3Ru",
        "outputId": "9ea6c3d4-97df-4871-ec41-007307c41d73"
      },
      "outputs": [],
      "source": [
        "train_df[(train_df['ProductBKey'] == random_item_id) & (train_df['CustomerBKey'] == random_store_id)].tail(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df[(train_df['CustomerBKey'] == random_item_id)].head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK8mg78KZJ7i",
        "outputId": "91562a49-43ca-47af-ecfe-9d5b7fd19271"
      },
      "outputs": [],
      "source": [
        "zero_count = (test_df['OrderWeight'] == 0).sum()\n",
        "print(f\"Number of predicted_sales == 0: {zero_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "HorqZSvohVUH",
        "outputId": "4c1f5a84-85fc-44f2-f363-421e0501c57a"
      },
      "outputs": [],
      "source": [
        "def zero_15_of_every_21_rows(df, target_col='OrderWeight'):\n",
        "    df1 = df.copy()\n",
        "    total_rows = len(df1)\n",
        "\n",
        "    for start in range(0, total_rows, 21):\n",
        "        end = min(start + 21, total_rows)\n",
        "        block_indices = list(range(start, end))\n",
        "        if len(block_indices) < 12:\n",
        "            continue  # skip small leftover blocks\n",
        "\n",
        "        # Randomly pick 15 indices in the block to zero out\n",
        "        zero_indices = np.random.choice(block_indices, size=12, replace=False)\n",
        "        df1.loc[zero_indices, target_col] = 0\n",
        "\n",
        "    return df1\n",
        "test_df_zero = zero_15_of_every_21_rows(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLM_6jBmZ18I"
      },
      "outputs": [],
      "source": [
        "test_df.to_parquet(\"Submission.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df_zero.to_parquet(\"Submissionzero.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df['OrderWeight'] *= 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "unique_test_pairs = test_df[['CustomerBKey', 'ProductBKey']].drop_duplicates().values.tolist()\n",
        "\n",
        "if unique_test_pairs:\n",
        "    random_pair = random.choice(unique_test_pairs)\n",
        "    random_store_id, random_item_id = random_pair\n",
        "\n",
        "    print(f\"\\nRandomly selected pair: CustomerID = {random_store_id}, Item ID = {random_item_id}\")\n",
        "\n",
        "    pair_df = test_df_zero[\n",
        "        (test_df_zero['CustomerBKey'] == random_store_id) &\n",
        "        (test_df_zero['ProductBKey'] == random_item_id)\n",
        "    ].copy()\n",
        "\n",
        "    item = pair_df[\"ProductForPlan10\"].iloc[0]\n",
        "    pair_df_21_days = pair_df.head(21)\n",
        "\n",
        "    # --- Plot using Plotly ---\n",
        "    fig = px.line(\n",
        "        pair_df_21_days,\n",
        "        x='OrderDate',\n",
        "        y='OrderWeight',\n",
        "        title=f'Store ID: {random_store_id}, Item ID: {random_item_id} - Predicted Sales (21 Days), item: {item}',\n",
        "        labels={'OrderDate': 'Date', 'OrderWeight': 'Predicted Sales'}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Predicted Sales',\n",
        "        xaxis_tickangle=-45,\n",
        "        template='plotly_white'\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo unique store-item pairs found in the test data to plot.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df[train_df['ProductBKey'] == random_item_id].head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_count = (test_df_zero['OrderWeight'] == 0).sum()\n",
        "print(f\"Number of predicted_sales == 0: {zero_count}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
