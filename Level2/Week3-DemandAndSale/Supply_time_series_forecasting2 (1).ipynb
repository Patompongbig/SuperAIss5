{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfxi7lJ7KlcW"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move kaggle.json to the correct folder\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!kaggle competitions download -c q-2-superai-ss-5-cp-axtra\n",
        "!mkdir -p /content/kaggle_competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH02ZyqhLKHK"
      },
      "outputs": [],
      "source": [
        "!unzip -q q-2-superai-ss-5-cp-axtra.zip -d /content/kaggle_competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTD4OcCcKwDh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu6TMhwbKxqk"
      },
      "outputs": [],
      "source": [
        "#his_sale1 = pd.read_csv('/content/kaggle_competition/historical_sale_2023_1.csv')\n",
        "#his_sale2 = pd.read_csv('/content/kaggle_competition/historical_sale_2023_2.csv')\n",
        "#his_sale3 = pd.read_csv('/content/kaggle_competition/historical_sale_2024_1.csv')\n",
        "#his_sale4 = pd.read_csv('/content/kaggle_competition/historical_sale_2024_2.csv')\n",
        "his_sale5 = pd.read_csv('/content/kaggle_competition/historical_sale_2025_1.csv')\n",
        "calendar = pd.read_csv('/content/kaggle_competition/calendar_date.csv')\n",
        "item = pd.read_csv('/content/kaggle_competition/item_hierarchy.csv')\n",
        "store = pd.read_csv('/content/kaggle_competition/store_data.csv')\n",
        "submis = pd.read_csv('/content/kaggle_competition/Q2_sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MwSMPQNLquw"
      },
      "outputs": [],
      "source": [
        "print(his_sale5.head(15))\n",
        "print(his_sale5.shape)\n",
        "print(his_sale5.nunique())\n",
        "print('\\n')\n",
        "print(calendar.head(5))\n",
        "print(calendar.shape)\n",
        "print(calendar.nunique())\n",
        "print('\\n')\n",
        "print(item.head(5))\n",
        "print(item.shape)\n",
        "print(item.nunique())\n",
        "print('\\n')\n",
        "print(store.head(5))\n",
        "print(store.shape)\n",
        "print('\\n')\n",
        "print(submis.head(5))\n",
        "print(submis.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2GSKjRCmRO6"
      },
      "outputs": [],
      "source": [
        "print(submis.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##เตรียม Full data สำหรับการ Rule-Based\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enBXD2AIVFCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqwmt0gU1v88"
      },
      "outputs": [],
      "source": [
        "def compare_item_ids(df1, df2):\n",
        "    \"\"\"\n",
        "    Compares the 'item_id' column of two pandas DataFrames and identifies items that are not present in both.\n",
        "\n",
        "    Args:\n",
        "        df1 (pd.DataFrame): The first DataFrame.\n",
        "        df2 (pd.DataFrame): The second DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two pandas Series:\n",
        "            - items_not_in_df2: Items present in df1 but not in df2.\n",
        "            - items_not_in_df1: Items present in df2 but not in df1.\n",
        "    \"\"\"\n",
        "    items_df1 = set(df1['item_id'])\n",
        "    items_df2 = set(df2['item_id'])\n",
        "\n",
        "    not_in_df2 = pd.Series(list(items_df1 - items_df2), name='item_id')\n",
        "    not_in_df1 = pd.Series(list(items_df2 - items_df1), name='item_id')\n",
        "    in_both = list(items_df1.intersection(items_df2))\n",
        "    print(len(in_both))\n",
        "    print(not_in_df2.count())\n",
        "    print(not_in_df1.count())\n",
        "\n",
        "    return not_in_df2, in_both\n",
        "\n",
        "not_in_df2, in_both = compare_item_ids(submis, his_sale5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reo_TlT3IYUw"
      },
      "outputs": [],
      "source": [
        "# Get the set of stores\n",
        "stores = set(submis['store_id'])\n",
        "\n",
        "# Create DataFrames from not_in_df2 and stores\n",
        "df_items = pd.DataFrame({'item_id': not_in_df2})\n",
        "df_stores = pd.DataFrame({'store_id': list(stores)})\n",
        "\n",
        "# Cartesian product using merge with 'cross' join (requires pandas 1.2+)\n",
        "mapped_df = df_items.merge(df_stores, how='cross')\n",
        "\n",
        "df_items = pd.DataFrame({'item_id': in_both})\n",
        "\n",
        "# Cartesian product using merge with 'cross' join (requires pandas 1.2+)\n",
        "mapped_intersect_df = df_items.merge(df_stores, how='cross')\n",
        "\n",
        "print(mapped_df.head())\n",
        "print(mapped_df.shape)  # should be (176640, 2)\n",
        "print(mapped_intersect_df.head())\n",
        "print(mapped_intersect_df.shape)  # should be (176640, 2)\n",
        "\n",
        "mapped_df = mapped_df.merge(\n",
        "    submis[['item_id', 'store_id']],\n",
        "    on=['item_id', 'store_id'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "mapped_intersect_df = mapped_intersect_df.merge(\n",
        "    submis[['item_id', 'store_id']],\n",
        "    on=['item_id', 'store_id'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "mapped_df = mapped_df.drop_duplicates()\n",
        "mapped_intersect_df = mapped_intersect_df.drop_duplicates()\n",
        "\n",
        "print(mapped_df.head())\n",
        "print(mapped_df.shape)  # should be (176640, 2)\n",
        "print(mapped_intersect_df.head())\n",
        "print(mapped_intersect_df.shape)  # should be (176640, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D43sLXH77sl9"
      },
      "outputs": [],
      "source": [
        "his_sale5['date'] = pd.to_datetime(his_sale5['date'])\n",
        "his_sale5 = his_sale5.sort_values(by='date')\n",
        "print(his_sale5.head())\n",
        "print(his_sale5.tail())\n",
        "print(his_sale5.shape)\n",
        "\n",
        "min_date_sale = his_sale5['date'].min() + pd.Timedelta(days=54)\n",
        "max_date_sale = his_sale5['date'].max()\n",
        "print(min_date_sale)\n",
        "print(max_date_sale)\n",
        "calendar['date'] = pd.to_datetime(calendar['date'])\n",
        "calendar_trimmed = calendar[(calendar['date'] >= min_date_sale) & (calendar['date'] <= max_date_sale)].copy()\n",
        "calendar_trimmed = calendar_trimmed.sort_values(by='date')\n",
        "calendar_trimmed = calendar_trimmed[['date', 'week', 'holiday']].reset_index(drop=True)\n",
        "print(calendar_trimmed.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4ov7-LsJ70C"
      },
      "outputs": [],
      "source": [
        "full_df1 = mapped_df.merge(calendar_trimmed, how='cross')\n",
        "full_df1['sales'] = 0\n",
        "full_df1['units_sold'] = 0.0\n",
        "\n",
        "full_df2 = mapped_intersect_df.merge(calendar_trimmed, how='cross')\n",
        "full_df2['sales'] = 0\n",
        "full_df2['units_sold'] = 0.0\n",
        "\n",
        "# Reorder columns to match your target format\n",
        "full_df1 = full_df1[['date', 'store_id', 'item_id', 'sales', 'units_sold']]\n",
        "full_df2 = full_df2[['date', 'store_id', 'item_id', 'sales', 'units_sold']]\n",
        "\n",
        "full_df = pd.concat([full_df1, full_df2], ignore_index=True)\n",
        "full_df = full_df.sort_values(by=['item_id', 'date'])\n",
        "\n",
        "print(full_df.head())\n",
        "print(full_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qRo3IY3--Sz"
      },
      "outputs": [],
      "source": [
        "sale_trim = his_sale5[(his_sale5['date'] >= min_date_sale)].copy()\n",
        "print(sale_trim.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Tlq1ixNHn_"
      },
      "outputs": [],
      "source": [
        "full_df = full_df.merge(sale_trim, on=['date', 'store_id', 'item_id'], how='left', suffixes=('', '_real'))\n",
        "\n",
        "# Replace default 0s with real values where available\n",
        "full_df['sales'] = full_df['sales_real'].fillna(full_df['sales'])\n",
        "full_df['units_sold'] = full_df['units_sold_real'].fillna(full_df['units_sold'])\n",
        "\n",
        "# Drop the temporary columns\n",
        "full_df.drop(columns=['sales_real', 'units_sold_real'], inplace=True)\n",
        "\n",
        "full_df = pd.merge(full_df, calendar_trimmed[['date', 'week']], on='date', how='left')\n",
        "\n",
        "print(full_df.head())\n",
        "print(full_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "696PZh5_aM4r"
      },
      "outputs": [],
      "source": [
        "full_df = full_df.sort_values(by=['item_id', 'store_id', 'date']).reset_index(drop=True)\n",
        "print(full_df.head(20))\n",
        "print(full_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_df.tail(20))\n"
      ],
      "metadata": {
        "id": "_DH1EJgrc078"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cazdw-PSqieB"
      },
      "outputs": [],
      "source": [
        "weekly_df = full_df.groupby(['store_id', 'item_id', 'week'], as_index=False).agg({\n",
        "    'sales': 'sum',\n",
        "    'units_sold': 'sum'\n",
        "})\n",
        "weekly_df = weekly_df.sort_values(by=['item_id', 'store_id']).reset_index(drop=True)\n",
        "\n",
        "print(weekly_df.nunique())\n",
        "print(weekly_df.head(30))\n",
        "print(weekly_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of times each (store_id, item_id) appears\n",
        "submis_counts = submis.groupby(['store_id', 'item_id']).size().reset_index(name='count')\n",
        "\n",
        "# Count how many pairs have exactly 1, 2, 3, ... entries\n",
        "for n in range(1, 11):\n",
        "    count_n = (submis_counts['count'] == n).sum()\n",
        "    print(f\"Pairs with exactly {n} entries in submis: {count_n}\")\n"
      ],
      "metadata": {
        "id": "BjO6qjJc1k3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_df_valid = weekly_df.groupby(['store_id', 'item_id']).filter(lambda x: len(x) >= 4)\n",
        "\n",
        "print(\"Pairs with at least 4 entries in weekly_df:\", weekly_df_valid[['store_id', 'item_id']].drop_duplicates().shape[0])\n"
      ],
      "metadata": {
        "id": "RIjYjl983fFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##จัดเตรียม Test Set\n"
      ],
      "metadata": {
        "id": "1aO2av4DWVBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = full_df.drop(columns=['sales', 'units_sold'])\n",
        "test_data['date'] = test_data['date'] + pd.Timedelta(days=28)\n",
        "test_data['week'] += 4\n",
        "\n",
        "print(test_data.head(20))\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "9XKXMvqrWaXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.tail()"
      ],
      "metadata": {
        "id": "jht0Vrwyclzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv('test_data_for_predict.csv', index=False)"
      ],
      "metadata": {
        "id": "Xso9K-APvt6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ทดสอบ"
      ],
      "metadata": {
        "id": "1iUIEry2byiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['sales_predict'] = 0\n",
        "test_data['units_sold_predict'] = 0\n",
        "\n",
        "test_data = test_data.groupby(['store_id', 'item_id', 'week'], as_index=False).agg({\n",
        "    'sales_predict': 'sum',\n",
        "    'units_sold_predict': 'sum'\n",
        "})\n",
        "test_data = test_data.sort_values(by=['item_id', 'store_id']).reset_index(drop=True)\n",
        "test_data = test_data.rename(columns={'week': 'week_predict'})\n",
        "\n",
        "print(test_data.nunique())\n",
        "print(test_data.head(30))\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "eWxGf_mjb8YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Label\n"
      ],
      "metadata": {
        "id": "FecQTFSbgh9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('/content/submission.csv')\n",
        "\n",
        "new_column_names = {\n",
        "    'store_id': 'store_id_check',\n",
        "    'item_id': 'item_id_check',\n",
        "    'week': 'week_check',\n",
        "    'units_sold_predict': 'predicted_sales'\n",
        "}\n",
        "\n",
        "print(test_data.head())\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "bQDuHnz1hs_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.rename(columns=new_column_names)\n",
        "\n",
        "# Keep only the renamed columns\n",
        "test_data = test_data[list(new_column_names.values())]\n",
        "\n",
        "# Check the result\n",
        "print(test_data.head())\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "bqqxbo8HuRmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.groupby(['store_id_check', 'item_id_check', 'week_check'], as_index=False).agg({\n",
        "    'predicted_sales': 'sum'\n",
        "})\n",
        "test_data = test_data.sort_values(by=['item_id_check', 'store_id_check']).reset_index(drop=True)\n",
        "test_data = test_data.rename(columns={'week_check': 'week_predict'})\n",
        "\n",
        "print(test_data.nunique())\n",
        "print(test_data.head(30))\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "4_q36xhCn-AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.describe()"
      ],
      "metadata": {
        "id": "eydZJ5NPv1dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(test_data['predicted_sales'], bins=500, kde=True)\n",
        "plt.title(\"Distribution of Predicted Sales\")\n",
        "plt.xlabel(\"Predicted Sales\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V60dQaq3u8e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data = pd.concat([weekly_df, test_data], axis=1)\n",
        "print(label_data.head())\n",
        "print(label_data.shape)\n",
        "print(label_data.isnull().sum)"
      ],
      "metadata": {
        "id": "Dg7pJI6CeWcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data.head(30)"
      ],
      "metadata": {
        "id": "wdD5uMqRzP1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data.loc[label_data['predicted_sales'] < 1.5, 'predicted_sales'] = 0"
      ],
      "metadata": {
        "id": "YpKBX6uojukP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create conditions and choices\n",
        "conditions = [\n",
        "    (label_data['units_sold'] > 0) & (label_data['predicted_sales'] == 0),  # Delete condition\n",
        "    (label_data['units_sold'] == 0) & (label_data['predicted_sales'] > 0)   # Add condition\n",
        "]\n",
        "\n",
        "choices = ['Delete', 'Add']\n",
        "\n",
        "# Apply conditions using np.select\n",
        "label_data['label'] = np.select(conditions, choices, default='Remain')\n",
        "\n",
        "print(label_data.head())"
      ],
      "metadata": {
        "id": "F7v-idQKgjmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_data[31:60])\n",
        "print(label_data.shape)"
      ],
      "metadata": {
        "id": "iE8vqKe4hiEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data['label'].value_counts().reset_index(name='count').rename(columns={'index': 'label'})\n"
      ],
      "metadata": {
        "id": "WBpkab9idBLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Submission"
      ],
      "metadata": {
        "id": "SQS-akp5k89w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare the label data: Select only necessary columns for the merge\n",
        "#    Make sure to select 'week_predict' (the correct column name)\n",
        "label_info = label_data[['store_id', 'item_id', 'week_predict', 'label']].copy()\n",
        "\n",
        "# Optional: Drop duplicates in label_info if there's a possibility of multiple labels\n",
        "# for the same store/item/week_predict combination. Keep the first one found.\n",
        "# label_info = label_info.drop_duplicates(subset=['store_id', 'item_id', 'week_predict'], keep='first')\n",
        "\n",
        "# 2. Perform a left merge\n",
        "#    'submis' is the left DataFrame (we want to keep all its rows)\n",
        "#    'label_info' is the right DataFrame\n",
        "#    Use 'left_on' and 'right_on' to specify the different week column names\n",
        "submis_merged = submis.merge(\n",
        "    label_info,\n",
        "    how='left', # Keep all rows from 'submis'\n",
        "    left_on=['store_id', 'item_id', 'YEAR_WEEK_NUMBER'], # Keys in 'submis'\n",
        "    right_on=['store_id', 'item_id', 'week_predict']   # Corresponding keys in 'label_info'\n",
        ")\n",
        "\n",
        "# 3. Update 'ITEM_STATUS'\n",
        "#    Use combine_first: If 'label' (from the merge) is not null, use it.\n",
        "#    Otherwise, keep the original 'ITEM_STATUS'.\n",
        "submis_merged['ITEM_STATUS'] = submis_merged['label'].combine_first(submis_merged['ITEM_STATUS'])\n",
        "\n",
        "# 4. Clean up: Drop the columns added during the merge that are no longer needed\n",
        "#    This includes 'label' and 'week_predict' from the right DataFrame.\n",
        "submis_final = submis_merged.drop(columns=['label', 'week_predict'])\n",
        "\n",
        "# 'submis_final' now contains the updated ITEM_STATUS\n",
        "print(submis_final.head(30))\n",
        "print(submis_final.shape)\n",
        "\n",
        "submis_final.to_csv('submissionQ2.csv', index=False)"
      ],
      "metadata": {
        "id": "Ps572iuOlBU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submis_final.nunique())\n",
        "print(submis_final.isnull().sum())"
      ],
      "metadata": {
        "id": "wrQrHeZsoyC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}