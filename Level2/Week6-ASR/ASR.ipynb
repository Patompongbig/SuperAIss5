{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kZfFfz6qF37O",
   "metadata": {
    "editable": true,
    "id": "kZfFfz6qF37O",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Move kaggle.json to the correct folder\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!kaggle competitions download -c audio-understanding\n",
    "!mkdir -p /content/kaggle_competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7yntT1DGCnA",
   "metadata": {
    "id": "f7yntT1DGCnA"
   },
   "outputs": [],
   "source": [
    "!unzip -q /content/audio-understanding.zip -d /content/kaggle_competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxSI6b_LGDGr",
   "metadata": {
    "id": "sxSI6b_LGDGr"
   },
   "outputs": [],
   "source": [
    "!pip install librosa soundfile noisereduce torchaudio datasets evaluate transformers \\\n",
    "    speechbrain git+https://github.com/snakers4/silero-vad.git plotly PyWavelets scipy\n",
    "!pip install --upgrade voicefixer\n",
    "!pip install jiwer\n",
    "!pip install voicefixer\n",
    "!pip install webrtcvad\n",
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "TdoOn_OEGaBU",
   "metadata": {
    "id": "TdoOn_OEGaBU"
   },
   "outputs": [],
   "source": [
    "# import webrtcvad\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, Audio\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import collections\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import noisereduce as nr\n",
    "import pywt\n",
    "import jiwer\n",
    "\n",
    "from numpy.fft import fft, fftfreq\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from datasets import load_dataset, Audio, ClassLabel, DatasetDict\n",
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "from speechbrain.pretrained import SepformerSeparation as separator\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline\n",
    ")\n",
    "import speechbrain\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import dolphin\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P1rsMOOj52lz",
   "metadata": {
    "id": "P1rsMOOj52lz"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DOc6R_-8LmjR",
   "metadata": {
    "id": "DOc6R_-8LmjR"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kdFm2r03VP8k",
   "metadata": {
    "id": "kdFm2r03VP8k"
   },
   "outputs": [],
   "source": [
    "folder = \"/content/kaggle_competition/speechs/speechs/test\"\n",
    "wav_files = [f for f in os.listdir(folder) if f.endswith(\".wav\")]\n",
    "random_file = random.choice(wav_files)\n",
    "\n",
    "print(random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "njNuUichLn07",
   "metadata": {
    "id": "njNuUichLn07"
   },
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(os.path.join(folder, random_file))\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pYpQLn7mMNMC",
   "metadata": {
    "id": "pYpQLn7mMNMC"
   },
   "outputs": [],
   "source": [
    "waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dVhjrvrMNhDC",
   "metadata": {
    "id": "dVhjrvrMNhDC"
   },
   "outputs": [],
   "source": [
    "wave = waveform[0].numpy()\n",
    "x = np.arange(len(wave)) / 16000  # time axis in seconds\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x, y=wave, mode='lines', name='Waveform'))\n",
    "fig.update_layout(title='Audio Waveform', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0l7ToAWnOLc0",
   "metadata": {
    "id": "0l7ToAWnOLc0"
   },
   "outputs": [],
   "source": [
    "Audio(waveform.numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L4qtS1fAFjDI",
   "metadata": {
    "id": "L4qtS1fAFjDI"
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def visualize_sound_3in1(waveform, waveclean, sample_rate=16000):\n",
    "    waveform = waveform.squeeze()\n",
    "    waveclean = waveclean.squeeze()\n",
    "\n",
    "    # 1. Time domain comparison\n",
    "    x = np.linspace(0, len(waveform) / sample_rate, len(waveform))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=waveform, mode='lines', name='Original'))\n",
    "    fig.add_trace(go.Scatter(x=x, y=waveclean, mode='lines', name='Filtered'))\n",
    "    fig.update_layout(title='Filtered Audio (Time Domain)', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "    fig.show()\n",
    "\n",
    "    # 2. Frequency domain\n",
    "    fft_output = torch.fft.rfft(waveclean)\n",
    "    magnitude_spectrum = torch.abs(fft_output)\n",
    "    num_bins = len(magnitude_spectrum)\n",
    "    frequencies = torch.linspace(0, sample_rate / 2, num_bins)\n",
    "    fig = go.Figure(data=go.Scatter(x=frequencies.numpy(), y=magnitude_spectrum.numpy(), mode='lines'))\n",
    "    fig.update_layout(\n",
    "        title='FFT Magnitude Spectrum (Filtered)',\n",
    "        xaxis_title='Frequency (Hz)',\n",
    "        yaxis_title='Magnitude',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 3. Playback (only works in Jupyter/IPython)\n",
    "    display(Audio(waveform, rate=sample_rate))\n",
    "    display(Audio(waveclean, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6lnTEdN3OanS",
   "metadata": {
    "id": "6lnTEdN3OanS"
   },
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ASf-IzZkOZ_1",
   "metadata": {
    "id": "ASf-IzZkOZ_1"
   },
   "outputs": [],
   "source": [
    "waveform = waveform.squeeze()\n",
    "\n",
    "fft_output = torch.fft.rfft(waveform)\n",
    "\n",
    "magnitude_spectrum = torch.abs(fft_output)\n",
    "\n",
    "num_bins = len(magnitude_spectrum)\n",
    "frequencies = torch.linspace(0, sample_rate / 2, num_bins)\n",
    "\n",
    "# Convert to numpy for Plotly (Plotly can handle tensors too, but numpy is common)\n",
    "frequencies_np = frequencies.numpy()\n",
    "magnitude_spectrum_np = magnitude_spectrum.numpy()\n",
    "\n",
    "# 4. Plotting with Plotly\n",
    "fig = go.Figure(data=go.Scatter(x=frequencies_np, y=magnitude_spectrum_np, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='FFT Magnitude Spectrum',\n",
    "    xaxis_title='Frequency (Hz)',\n",
    "    yaxis_title='Magnitude',\n",
    "    hovermode='x unified', # Shows a tooltip across the x-axis for all traces\n",
    "    template='plotly_white' # A clean white background template\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_xu3uJrYIVLM",
   "metadata": {
    "id": "_xu3uJrYIVLM"
   },
   "source": [
    "## Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Jh1so62FLeA",
   "metadata": {
    "id": "3Jh1so62FLeA"
   },
   "source": [
    "### Bandpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63TIhDwuFYrK",
   "metadata": {
    "id": "63TIhDwuFYrK"
   },
   "outputs": [],
   "source": [
    "#Band pass filter function\n",
    "def get_dominant_frequency(signal, sample_rate=16000):\n",
    "    n = len(signal)\n",
    "    yf = np.abs(fft(signal))[:n // 2]  # Take positive frequencies\n",
    "    xf = fftfreq(n, 1 / sample_rate)[:n // 2]  # Frequency bins\n",
    "\n",
    "    idx = np.argmax(yf)  # Index of max amplitude\n",
    "    dominant_freq = xf[idx]\n",
    "    return dominant_freq\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pGdrNmZAFk5d",
   "metadata": {
    "id": "pGdrNmZAFk5d"
   },
   "outputs": [],
   "source": [
    "wavelet_fft = waveform.squeeze()\n",
    "\n",
    "highest_freq = get_dominant_frequency(wavelet_fft, 16000)\n",
    "lowcut = max(20, highest_freq - 1000)\n",
    "wavelet_clean = apply_bandpass_filter(wavelet_fft, lowcut, highest_freq + 2000, sample_rate)\n",
    "\n",
    "visualize_sound_3in1(wavelet_fft, wavelet_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZbtpvoVnFTcq",
   "metadata": {
    "id": "ZbtpvoVnFTcq"
   },
   "source": [
    "### NoiseReducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oFNdkAfBGGes",
   "metadata": {
    "id": "oFNdkAfBGGes"
   },
   "outputs": [],
   "source": [
    "wavelet_fft = waveform.squeeze()\n",
    "\n",
    "wavelet_clean = nr.reduce_noise(y=wavelet_fft, sr=sample_rate, prop_decrease=0.95, stationary=False, use_tqdm=False, n_fft=2048, win_length=None, hop_length=512)\n",
    "visualize_sound_3in1(wavelet_fft, wavelet_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H7_-fhiVFPhA",
   "metadata": {
    "id": "H7_-fhiVFPhA"
   },
   "source": [
    "### Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nw39zhPjGSLN",
   "metadata": {
    "id": "Nw39zhPjGSLN"
   },
   "outputs": [],
   "source": [
    "def wavelet_denoise(data, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "\n",
    "    sigma = np.median(np.abs(coeff[-level])) / 0.6745\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(data))) *0.8\n",
    "\n",
    "    coeff[1:] = [pywt.threshold(i, value=uthresh, mode='soft') for i in coeff[1:]]\n",
    "    return pywt.waverec(coeff, wavelet, mode=\"per\")\n",
    "\n",
    "waveform_fft = waveform.squeeze()\n",
    "\n",
    "wavelet_clean = wavelet_denoise(waveform_fft)\n",
    "visualize_sound_3in1(waveform_fft, wavelet_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NkBimWKM7nWs",
   "metadata": {
    "id": "NkBimWKM7nWs"
   },
   "source": [
    "### Mel-Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bER55hNc78zC",
   "metadata": {
    "id": "bER55hNc78zC"
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = librosa.stft(waveform[0].numpy())\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "\n",
    "# Before\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4UNfg_TfBf0P",
   "metadata": {
    "id": "4UNfg_TfBf0P"
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = librosa.stft(waveform_clean.numpy())\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "# Before\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QQ6MYq7C0gF",
   "metadata": {
    "id": "_QQ6MYq7C0gF"
   },
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=waveform_clean.numpy(), sr=16000, n_fft=2048, hop_length=512, n_mels=128)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pbapL-hLG8uH",
   "metadata": {
    "id": "pbapL-hLG8uH"
   },
   "source": [
    "## Speechmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CtJMrwAGIVLP",
   "metadata": {
    "id": "CtJMrwAGIVLP"
   },
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kt6igeB5IVLP",
   "metadata": {
    "id": "Kt6igeB5IVLP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "OzP6SycvIVLP",
   "metadata": {
    "id": "OzP6SycvIVLP"
   },
   "source": [
    "### Speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4k2zJGhOIXT",
   "metadata": {
    "id": "a4k2zJGhOIXT"
   },
   "outputs": [],
   "source": [
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-5tjaP0eQQ4k",
   "metadata": {
    "id": "-5tjaP0eQQ4k"
   },
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load('/content/kaggle_competition/speechs/speechs/test/682ab42b-441c-4089-a508-96fda0104e35.wav')\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nnbxrIxZIVLQ",
   "metadata": {
    "id": "nnbxrIxZIVLQ"
   },
   "outputs": [],
   "source": [
    "# Load enhancement model\n",
    "enhancer = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"speechbrain/metricgan-plus-voicebank\",\n",
    "    savedir=\"tmpdir_metricgan\"\n",
    ")\n",
    "# Enhance\n",
    "# Assume waveform has shape [1, time]\n",
    "lengths = torch.tensor([1.0])  # Full length used\n",
    "\n",
    "# Enhance the audio\n",
    "wavelet_clean = enhancer.enhance_batch(waveform, lengths)\n",
    "\n",
    "\n",
    "visualize_sound_3in1(waveform, wavelet_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6129d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Tb7eahPoHEWd",
   "metadata": {
    "id": "Tb7eahPoHEWd"
   },
   "source": [
    "### VoiceFixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vzGcp1zBHHp3",
   "metadata": {
    "id": "vzGcp1zBHHp3"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "voicefixer = VoiceFixer()\n",
    "\n",
    "# Download pretrained model if not already downloaded\n",
    "voicefixer.load_model()\n",
    "\n",
    "# Set your file paths\n",
    "input_path = \"thai_input.wav\"      # Path to your Thai audio\n",
    "output_path = \"thai_output.wav\"    # Path to save enhanced audio\n",
    "\n",
    "# Run inference (restore)\n",
    "voicefixer.restore(input_path, output_path, cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NS81VDNU6FNP",
   "metadata": {
    "id": "NS81VDNU6FNP"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2HfJszz7EyZR",
   "metadata": {
    "id": "2HfJszz7EyZR"
   },
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "VhNIFoQ9DxRQ",
   "metadata": {
    "id": "VhNIFoQ9DxRQ"
   },
   "outputs": [],
   "source": [
    "#Wavelet function\n",
    "def wavelet_denoise(data, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(data, wavelet, mode=\"per\")\n",
    "\n",
    "    sigma = np.median(np.abs(coeff[-level])) / 0.6745\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(len(data))) *0.8\n",
    "\n",
    "    coeff[1:] = [pywt.threshold(i, value=uthresh, mode='soft') for i in coeff[1:]]\n",
    "    return pywt.waverec(coeff, wavelet, mode=\"per\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SdaXKQkvD1sG",
   "metadata": {
    "id": "SdaXKQkvD1sG"
   },
   "outputs": [],
   "source": [
    "#Band pass filter function\n",
    "def get_dominant_frequency(signal, sample_rate=16000):\n",
    "    n = len(signal)\n",
    "    yf = np.abs(fft(signal))[:n // 2]  # Take positive frequencies\n",
    "    xf = fftfreq(n, 1 / sample_rate)[:n // 2]  # Frequency bins\n",
    "\n",
    "    idx = np.argmax(yf)  # Index of max amplitude\n",
    "    dominant_freq = xf[idx]\n",
    "    return dominant_freq\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "h8j8fEOnQ8ZS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309,
     "referenced_widgets": [
      "349476fc8c484293b015fd92e4ad2db7",
      "1ce8c60590e14910afaa6ce00abe0794",
      "461376f4e76742058891431c89a78f8f",
      "9574c555216d4babb7cc708e645b009d",
      "7c44c32c5efd48ffa96ea89f80a78057",
      "3992903e76d3409aba5453b9b595ae74",
      "2d83bf8159ce498b98fcdc93ff0755a7",
      "cb991ede13dd4d92a321a76af18ca953",
      "1135f9768e0b4069a9670635c7ba3d4a",
      "730d9995bf8943c1a6176ef5ed2b65f9",
      "dcfa4273a6ce4c43966dafaa8b83f298",
      "f2d5c233f6054510b6e96f4034f64f99",
      "493f2779d8414872b6827de2f5c5ce6e",
      "ffb0332927d84426b092b6b1442b2164",
      "a346eebc36fc453d9c84f7e45ec9d213",
      "9f613731e2644c67bfe69932250c9893",
      "a2a1cf2a678944efabf588855de97052",
      "f544594c534e4df7808679139a8ec36c",
      "0c791789d637407f8d56f931d4d6d90a",
      "81e83ef180fd416aabecb0627d3f4948",
      "a4d951fc37524dd7974999d731c41d2b",
      "8986a6ffa20d4a6c80aa4d0592ef28b3"
     ]
    },
    "id": "h8j8fEOnQ8ZS",
    "outputId": "f1ee6b99-83b8-47dd-d3be-c265cecca2fe"
   },
   "outputs": [],
   "source": [
    "enhancer = SpectralMaskEnhancement.from_hparams(\n",
    "    source=\"/project/ai901504-ai0004/501641_Big/week4/metricgan-plus-voicebank\",\n",
    "    savedir=\"/project/ai901504-ai0004/501641_Big/week4/tmpdir_metricgan\",\n",
    "    run_opts={\"device\": str('cuda')}\n",
    ")\n",
    "\n",
    "def deep_speech(waveform):\n",
    "\n",
    "  if isinstance(waveform, np.ndarray):\n",
    "        waveform = torch.from_numpy(waveform)\n",
    "  if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)  # [1, N]\n",
    "  waveform = waveform.float().to(device)  # float32\n",
    "\n",
    "  # Assume waveform has shape [1, time]\n",
    "  lengths = torch.tensor([1.0]).to(device)  # Full length used\n",
    "\n",
    "  # Enhance the audio\n",
    "  wavelet_clean = enhancer.enhance_batch(waveform, lengths)\n",
    "\n",
    "  return wavelet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "WFKKncZYLZe-",
   "metadata": {
    "id": "WFKKncZYLZe-"
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "def voice_fixer(waveform):\n",
    "    if isinstance(waveform, np.ndarray):\n",
    "        waveform_int16 = np.int16(waveform * 32767)\n",
    "        write(\"output.wav\", 16000, waveform_int16)\n",
    "    elif isinstance(waveform, torch.Tensor):\n",
    "        # Make sure tensor shape is [channels, samples]\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        torchaudio.save(\"output.wav\", waveform, sample_rate=16000)\n",
    "    else:\n",
    "        raise TypeError(\"Input waveform must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    voicefixer = VoiceFixer()\n",
    "    voicefixer.restore('/content/output.wav', '/content/process.wav', cuda=False)\n",
    "    waveform, sample_rate = torchaudio.load('/content/process.wav')\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XhBeGwxxD4oG",
   "metadata": {
    "id": "XhBeGwxxD4oG"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Visualization\n",
    "def visualize_sound_3in1(waveform, waveclean, sample_rate=16000):\n",
    "    waveform = waveform.squeeze()\n",
    "    waveclean = waveclean.squeeze()\n",
    "\n",
    "    # 1. Time domain comparison\n",
    "    x1 = np.linspace(0, len(waveform) / sample_rate, len(waveform))\n",
    "    x2 = np.linspace(0, len(waveclean) / sample_rate, len(waveclean))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x1, y=waveform.numpy(), mode='lines', name='Original'))\n",
    "    fig.add_trace(go.Scatter(x=x2, y=waveclean.numpy(), mode='lines', name='Filtered'))\n",
    "    fig.update_layout(title='Filtered Audio (Time Domain)', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "    fig.show()\n",
    "\n",
    "    # 2. Frequency domain\n",
    "    fft_output = torch.fft.rfft(waveclean)\n",
    "    magnitude_spectrum = torch.abs(fft_output)\n",
    "    num_bins = len(magnitude_spectrum)\n",
    "    frequencies = torch.linspace(0, sample_rate / 2, num_bins)\n",
    "    fig = go.Figure(data=go.Scatter(x=frequencies.numpy(), y=magnitude_spectrum.numpy(), mode='lines'))\n",
    "    fig.update_layout(\n",
    "        title='FFT Magnitude Spectrum (Filtered)',\n",
    "        xaxis_title='Frequency (Hz)',\n",
    "        yaxis_title='Magnitude',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    # 3. Playback (only works in Jupyter/IPython)\n",
    "    display(Audio(waveform, rate=sample_rate))\n",
    "    display(Audio(waveclean, rate=sample_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "_n64zsSYET91",
   "metadata": {
    "id": "_n64zsSYET91"
   },
   "outputs": [],
   "source": [
    "def normalize_waveform(waveform):\n",
    "    # Ensure waveform is a tensor\n",
    "    if not isinstance(waveform, torch.Tensor):\n",
    "        waveform = torch.tensor(waveform)\n",
    "\n",
    "    # Use torch.abs and torch.max for tensors\n",
    "    max_val = torch.max(torch.abs(waveform))\n",
    "    if max_val == 0:\n",
    "        return waveform  # Avoid division by zero\n",
    "    return waveform / max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0Ukb-486HBE",
   "metadata": {
    "id": "a0Ukb-486HBE"
   },
   "outputs": [],
   "source": [
    "def noise_reduce(waveform):\n",
    "  #Change to the right format\n",
    "  wavelet_clean = waveform.squeeze()\n",
    "\n",
    "  #2.Start with Wavelet\n",
    "  wavelet_clean = wavelet_denoise(wavelet_clean)\n",
    "\n",
    "  #3.Band pass filter\n",
    "  highest_freq = get_dominant_frequency(wavelet_clean, 16000)\n",
    "  lowcut = max(300, highest_freq - 1000)\n",
    "  wavelet_clean = apply_bandpass_filter(wavelet_clean, lowcut, max(4000, highest_freq + 1000), sample_rate)\n",
    "\n",
    "  #4.deep_speech\n",
    "  # wavelet_clean = deep_speech(wavelet_clean)\n",
    "\n",
    "  #5.noise_reducer\n",
    "  wavelet_clean = nr.reduce_noise(y=wavelet_clean, sr=sample_rate, prop_decrease=0.6, stationary=True, use_tqdm=False, n_fft=2048, win_length=1536, hop_length=512)\n",
    "\n",
    "  #6.normalize\n",
    "  wavelet_clean = normalize_waveform(wavelet_clean)\n",
    "\n",
    "  return wavelet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mSiKx94RiCxr",
   "metadata": {
    "id": "mSiKx94RiCxr"
   },
   "outputs": [],
   "source": [
    "def deep_noise_reduce(waveform):\n",
    "  #Change to the right format\n",
    "  wavelet_clean = waveform.squeeze()\n",
    "\n",
    "  #1.Start with Wavelet\n",
    "  wavelet_clean = wavelet_denoise(wavelet_clean)\n",
    "\n",
    "  #2.deep_speech\n",
    "  wavelet_clean = deep_speech(wavelet_clean)\n",
    "\n",
    "  #3.normalize\n",
    "  wavelet_clean = normalize_waveform(wavelet_clean)\n",
    "\n",
    "  return wavelet_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "\n",
    "ans = pipeline(\n",
    "    Tasks.acoustic_noise_suppression,\n",
    "    model='/project/ai901504-ai0004/501641_Big/week4/speech_dfsmn_ans_psm_48k_causal')\n",
    "result = ans(\n",
    "    '/content/speechs/speechs/test/1460a0f9-fbb6-4db1-a3af-582b93e79e6d.wav',\n",
    "    output_path='output.wav')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xftm0PBxE1Pz",
   "metadata": {
    "id": "Xftm0PBxE1Pz"
   },
   "source": [
    "### sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdA7882BIVLS",
   "metadata": {
    "id": "sdA7882BIVLS"
   },
   "outputs": [],
   "source": [
    "# waveform, sample_rate = torchaudio.load('/content/kaggle_competition/speechs/speechs/test/1460a0f9-fbb6-4db1-a3af-582b93e79e6d.wav')\n",
    "# resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "# waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LmwdApGhsY2N",
   "metadata": {
    "id": "LmwdApGhsY2N"
   },
   "outputs": [],
   "source": [
    "folder = \"/project/ai901504-ai0004/501641_Big/week4/speechs/speechs/test\"\n",
    "wav_files = [f for f in os.listdir(folder) if f.endswith(\".wav\")]\n",
    "random_file = random.choice(wav_files)\n",
    "\n",
    "print(random_file)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load(os.path.join(folder, random_file))\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform = resampler(waveform)\n",
    "\n",
    "waveform, sample_rate = torchaudio.load('/project/ai901504-ai0004/501641_Big/week4/speechs/speechs/test/4a656e78-62b6-4277-a30b-9cfedbc3a184.wav')\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8a34e",
   "metadata": {},
   "source": [
    "# ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d415a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#โหลดโมเดลDolphinจากไฟล์ก่อนหน้า\n",
    "import dolphin\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "model = dolphin.load_model(\"small\",\n",
    "                           \"/project/ai901504-ai0004/501641_Big/week4/DataoceanAI-dolphin-small\",\n",
    "                           \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"/project/ai901504-ai0004/501641_Big/week4/faster-whisper-large-v2-th\"\n",
    "lang = \"th\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    chunk_length_s=20,\n",
    "    device=device,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64b6851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pipeline(\n",
    "    Tasks.acoustic_noise_suppression,\n",
    "    model='/project/ai901504-ai0004/501641_Big/week4/speech_dfsmn_ans_psm_48k_causal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0749df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /project/ai901504-ai0004/501641_Big/week4/denoise\n",
    "!mkdir /project/ai901504-ai0004/501641_Big/week4/denoise\n",
    "!mkdir /project/ai901504-ai0004/501641_Big/week4/denoise/train\n",
    "!mkdir /project/ai901504-ai0004/501641_Big/week4/denoise/test\n",
    "!mkdir /project/ai901504-ai0004/501641_Big/week4/denoise/resample_train\n",
    "!mkdir /project/ai901504-ai0004/501641_Big/week4/denoise/resample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4762fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pl.read_csv('/project/ai901504-ai0004/501641_Big/week4/Human_Labor_train.csv')\n",
    "test_df = pl.read_csv('//project/ai901504-ai0004/501641_Big/week4/test.csv')\n",
    "\n",
    "predict_data = 'test'\n",
    "predict_df = test_df\n",
    "num_files = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b575a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = Path(f'/project/ai901504-ai0004/501641_Big/week4/speechs/speechs/{predict_data}').glob('*.wav')\n",
    "# input_paths = Path(f'/project/ai901504-ai0004/501641_Big/week4/noise_augment').glob('*.wav')\n",
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/{predict_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(input_paths, total=num_files):\n",
    "  waveform, sample_rate = torchaudio.load(str(path))\n",
    "  waveform = deep_noise_reduce(waveform)\n",
    "  waveform = waveform.cpu()\n",
    "  torchaudio.save(output_path/path.name, waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ae64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(input_paths, total=num_files):\n",
    "  waveform, sample_rate = torchaudio.load(str(path))\n",
    "  waveform = noise_reduce(waveform)\n",
    "  waveform = waveform.unsqueeze(0)\n",
    "  torchaudio.save(output_path/path.name, waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3799e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = Path(f'/project/ai901504-ai0004/501641_Big/week4/speechs/speechs/{predict_data}').glob('*.wav')\n",
    "resample_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/resample_{predict_data}')\n",
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/{predict_data}')\n",
    "\n",
    "# Step 1: Resample to 48kHz\n",
    "for path in tqdm(input_paths, total=num_files):\n",
    "    waveform, sample_rate = torchaudio.load(str(path))\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=48000)\n",
    "    waveform = resampler(waveform)\n",
    "    torchaudio.save(resample_path / path.name, waveform, 48000)\n",
    "\n",
    "# Step 2: Noise reduction\n",
    "resample_paths = list(resample_path.glob('*.wav'))\n",
    "for path in tqdm(resample_paths, total=num_files):\n",
    "    result = ans(str(path), output_path=resample_path / path.name)\n",
    "\n",
    "# Step 3: Resample back to 16kHz\n",
    "for path in tqdm(resample_paths, total=num_files):\n",
    "    waveform, sample_rate = torchaudio.load(str(path))\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    torchaudio.save(output_path / path.name, waveform, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe47a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/{predict_data}')\n",
    "# output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/noise_augment')\n",
    "\n",
    "output_paths = list(output_path.glob('*.wav')) \n",
    "\n",
    "file_names = [path.stem for path in output_paths]\n",
    "out_paths = [str(path) for path in output_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ddf7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_base = Path(f'/project/ai901504-ai0004/501641_Big/week4/speechs/speechs/{predict_data}')\n",
    "folder_base = Path(f'/project/ai901504-ai0004/501641_Big/week4/noise_augment')\n",
    "# folder_denoise = Path(\"/project/ai901504-ai0004/501641_Big/week4/noise_augment\")\n",
    "folder_denoise = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/{predict_data}')\n",
    "wav_files = [f for f in os.listdir(folder_base) if f.endswith(\".wav\")]\n",
    "random_file = random.choice(wav_files)\n",
    "\n",
    "print(random_file)\n",
    "\n",
    "waveform_base, sample_rate = torchaudio.load(os.path.join(folder_base, random_file))\n",
    "waveform_denoise, sample_rate = torchaudio.load(os.path.join(folder_denoise, random_file))\n",
    "\n",
    "# waveform_base, sample_rate = torchaudio.load('/project/ai901504-ai0004/501641_Big/week4/noise_augment/33466244-2697-4de2-81b1-196a56ffd6bb.wav')\n",
    "# waveform_denoise, sample_rate = torchaudio.load(f'/project/ai901504-ai0004/501641_Big/week4/denoise/train/33466244-2697-4de2-81b1-196a56ffd6bb.wav')\n",
    "\n",
    "visualize_sound_3in1(waveform_base, waveform_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_augment(waveform, sample_rate=16000, noise_factor=0.005):\n",
    "    if waveform.ndim == 2:\n",
    "        waveform = waveform[0]  # [1, N] -> [N]\n",
    "\n",
    "    noise = torch.randn_like(waveform)  # generate noise with same shape\n",
    "    augmented_waveform = waveform + noise_factor * noise\n",
    "    augmented_waveform = torch.clamp(augmented_waveform, -1.0, 1.0)\n",
    "\n",
    "    return augmented_waveform.unsqueeze(0) # [1, N]\n",
    "\n",
    "\n",
    "input_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/denoise/{predict_data}').glob('*.wav')\n",
    "\n",
    "for path in tqdm(input_path, total=num_files):\n",
    "  waveform, sample_rate = torchaudio.load(str(path))\n",
    "  waveform = noise_augment(waveform)\n",
    "  torchaudio.save(path, waveform, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd132f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11299a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8bd7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "i = 0\n",
    "\n",
    "for name in file_names:\n",
    "  result_dict[name] = result[i]['text']\n",
    "  i += 1\n",
    "\n",
    "predict_df = predict_df.with_columns(\n",
    "    pl.col(\"id\").replace(result_dict).alias(\"denoised_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a238eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674fcd6",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = predict_df['Gemini_Transcript ไว้แก้'].to_list()\n",
    "hypothesis = predict_df['denoised_text'].to_list()\n",
    "error_metrics = jiwer.compute_measures(references, hypothesis)\n",
    "error_metrics['wer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50400cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Thonburian\n",
    "#DeepSpeech\n",
    "#0.95\n",
    "\n",
    "#Signal Processing\n",
    "#0.91\n",
    "#0.944640753828033\n",
    "\n",
    "#Normal\n",
    "#1.07\n",
    "\n",
    "#Chinoise\n",
    "#0.9358068315665489\n",
    "\n",
    "#############################################\n",
    "##Pathumma\n",
    "#DeepSpeech\n",
    "#2.5\n",
    "\n",
    "#Signal Processing\n",
    "#3.6\n",
    "\n",
    "#Normal\n",
    "#0.95\n",
    "\n",
    "#Chinese\n",
    "#0.9567137809187279\n",
    "#0.9584805653710248\n",
    "\n",
    "#Chinoise\n",
    "#\n",
    "\n",
    "#################################################\n",
    "##Biodatlab\n",
    "#Normal\n",
    "#1.0032391048292109\n",
    "\n",
    "################################################\n",
    "##Monsoon\n",
    "#Normal\n",
    "#1.0\n",
    "\n",
    "#Signal Processing\n",
    "#1.0\n",
    "\n",
    "#Deepspeech\n",
    "#1.0\n",
    "################################################\n",
    "##whisper-th-large-combined\n",
    "#Deepspeech\n",
    "#0.9829210836277974\n",
    "\n",
    "#Signal Processing\n",
    "#0.9234393404004712\n",
    "\n",
    "#Normal\n",
    "#0.9511189634864546\n",
    "\n",
    "#Chinese\n",
    "#0.9390459363957597"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f730cb8",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bac231",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /ai901504-ai0004/501641_Big/week4/inference\n",
    "!mkdir /ai901504-ai0004/501641_Big/week4/inference\n",
    "!mkdir /ai901504-ai0004/501641_Big/week4/inference/Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /ai901504-ai0004/501641_Big/week4/inference/Deepspeech\n",
    "!mkdir /ai901504-ai0004/501641_Big/week4/inference/Deepspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8488bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"/project/ai901504-ai0004/501641_Big/week4/inference/Normal\")\n",
    "output_dir = Path(\"/project/ai901504-ai0004/501641_Big/week4/inference/Deepspeech\")\n",
    "\n",
    "for path in tqdm(list(input_dir.glob(\"*.wav\")), total=5):\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(str(path))\n",
    "        waveform = deep_noise_reduce(waveform)\n",
    "        waveform = waveform.cpu()\n",
    "        torchaudio.save(str(output_dir / path.name), waveform, sample_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /ai901504-ai0004/501641_Big/week4/inference/Signalprocessing\n",
    "!mkdir /ai901504-ai0004/501641_Big/week4/inference/Signalprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6448d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path(\"/project/ai901504-ai0004/501641_Big/week4/inference/Normal\")\n",
    "output_dir = Path(\"/project/ai901504-ai0004/501641_Big/week4/inference/Signalprocessing\")\n",
    "\n",
    "# Process audio files\n",
    "for path in tqdm(list(input_dir.glob(\"*.wav\")), total=5):\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(str(path))\n",
    "        waveform = noise_reduce(waveform)\n",
    "        waveform = waveform.unsqueeze(0)  # Ensure shape (1, N) for saving\n",
    "        torchaudio.save(str(output_dir / path.name), waveform, sample_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/Normal').glob('*.wav')\n",
    "resample_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/resample')\n",
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/denoise')\n",
    "\n",
    "# Step 1: Resample to 48kHz\n",
    "for path in tqdm(input_paths, total=num_files):\n",
    "    waveform, sample_rate = torchaudio.load(str(path))\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=48000)\n",
    "    waveform = resampler(waveform)\n",
    "    torchaudio.save(resample_path / path.name, waveform, 48000)\n",
    "\n",
    "# Step 2: Noise reduction\n",
    "resample_paths = list(resample_path.glob('*.wav'))\n",
    "for path in tqdm(resample_paths, total=num_files):\n",
    "    result = ans(str(path), output_path=resample_path / path.name)\n",
    "\n",
    "# Step 3: Resample back to 16kHz\n",
    "for path in tqdm(resample_paths, total=num_files):\n",
    "    waveform, sample_rate = torchaudio.load(str(path))\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "    waveform = resampler(waveform)\n",
    "    torchaudio.save(output_path / path.name, waveform, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_base = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/Normal')\n",
    "\n",
    "folder_noise = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/denoise')\n",
    "\n",
    "wav_files = [f for f in os.listdir(folder_base) if f.endswith(\".wav\")]\n",
    "random_file = random.choice(wav_files)\n",
    "\n",
    "print(random_file)\n",
    "\n",
    "waveform_base, sample_rate = torchaudio.load(os.path.join(folder_base, random_file))\n",
    "waveform_noise, sample_rate = torchaudio.load(os.path.join(folder_noise, random_file))\n",
    "\n",
    "visualize_sound_3in1(waveform_base, waveform_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d139b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/Normal')\n",
    "output_paths = list(output_path.glob('*.wav')) \n",
    "\n",
    "file_names = [path.stem for path in output_paths]\n",
    "out_paths = [str(path) for path in output_paths]\n",
    "\n",
    "result0 = pipe(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7604ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c927c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/Signalprocessing')\n",
    "output_paths = list(output_path.glob('*.wav')) \n",
    "\n",
    "file_names = [path.stem for path in output_paths]\n",
    "out_paths = [str(path) for path in output_paths]\n",
    "\n",
    "result1 = pipe(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(f'/project/ai901504-ai0004/501641_Big/week4/inference/Deepspeech')\n",
    "output_paths = list(output_path.glob('*.wav')) \n",
    "\n",
    "file_names = [path.stem for path in output_paths]\n",
    "out_paths = [str(path) for path in output_paths]\n",
    "\n",
    "result2 = pipe(out_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = librosa.stft(waveform_base[0].numpy())\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "\n",
    "# Before\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a543c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = librosa.stft(waveform_denoise[0].numpy())\n",
    "S_db = librosa.amplitude_to_db(np.abs(S), ref=np.max)\n",
    "# Before\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47add7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=waveform_denoise[0].numpy(), sr=16000, n_fft=2048, hop_length=512, n_mels=128)\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(S), ref=np.max),\n",
    "                         sr=16000, x_axis='time', y_axis='hz')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9554f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
