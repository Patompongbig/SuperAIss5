# -*- coding: utf-8 -*-
"""NER_LST20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSHySgIa0ae_TDr7nuwhEXXH1-RIyBDs
"""

import torch

# Check if CUDA (GPU support) is available
if torch.cuda.is_available():
    print("GPU is available!")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
else:
    print("GPU is not available.")

print(torch.version.cuda)

import os
import pandas as pd
import numpy as np

'''data_dir = '/content/data/train/train'
data = []

for file_name in os.listdir(data_dir):
  if file_name.endswith('.txt'):
    with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:
              for line in file:
                parts = line.strip().split('\t')
                if len(parts) == 4:
                  word, pos_tag, ner_tag, custom_tag = parts
                  data.append({"word":word, "pos_tag":pos_tag, "ner_tag":ner_tag, "custom_tag":custom_tag})

df = pd.DataFrame(data)

df.to_csv('processed_data.csv', index=False, encoding='utf-8')

print(df.head())'''

'''eval_folder = '/content/data/eval/eval'
eval_data = []

for file_name in os.listdir(eval_folder):
    if file_name.endswith('.txt'):  # Ensure only .txt files are processed
        with open(os.path.join(eval_folder, file_name), 'r', encoding='utf-8') as file:
            for line in file:
                # Split each line by tab
                parts = line.strip().split('\t')
                if len(parts) == 4:  # Ensure the format matches
                    word, pos_tag, ner_tag, custom_tag = parts
                    eval_data.append({"word": word, "pos_tag": pos_tag, "ner_tag": ner_tag, "custom_tag": custom_tag})

eval_df = pd.DataFrame(eval_data)

# Save the combined DataFrame to a CSV file (optional)
eval_df.to_csv('eval_combined.csv', index=False, encoding='utf-8')

print(eval_df.head())'''

tag_list_df = pd.read_csv('tag_list.csv')
print(tag_list_df)

def process_folder(folder_path, output_file):
    all_data = []

    # Iterate through all files in the folder
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.txt'):  # Ensure only .txt files are processed
            file_id = file_name.split('.')[0]  # Extract the folder name (file ID)

            # Read the file line by line
            with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:
                for idx, line in enumerate(file):
                    parts = line.strip().split('\t')  # Split the line into parts
                    if len(parts) == 4:  # Ensure the line has all 4 components
                        word, pos_tag, ner_tag, custom_tag = parts
                        folder_name_row = f"{file_id}_{idx}"
                        all_data.append({
                            "folder_name_row": folder_name_row,
                            "word": word,
                            "pos_tag": pos_tag,
                            "ner_tag": ner_tag,
                            "custom_tag": custom_tag
                        })

    df = pd.DataFrame(all_data)

    df.to_csv(output_file, index=False, encoding='utf-8')

    return df

train_folder = 'train/train'
eval_folder = 'eval/eval'

train_df = process_folder(train_folder, 'processed_train.csv')
eval_df = process_folder(eval_folder, 'processed_eval.csv')

print(train_df.head())
print(eval_df.head())

train_df[ train_df["folder_name_row"] == "00001_0"]

def join_ner_with_class(df, tag_list_df):
    # Merge on ner_tag (tag in tag_list_df)
    merged_df = df.merge(tag_list_df, left_on='ner_tag', right_on='tag', how='left')

    # Drop the extra 'tag' column (optional)
    merged_df = merged_df.drop(columns=['tag'])

    return merged_df

# Example usage
train_with_class_df = join_ner_with_class(train_df, tag_list_df)
eval_with_class_df = join_ner_with_class(eval_df, tag_list_df)

# Save the resulting DataFrame (optional)
train_with_class_df.to_csv('train_with_class.csv', index=False, encoding='utf-8')
eval_with_class_df.to_csv('eval_with_class.csv', index=False, encoding='utf-8')

# Print a sample of the updated DataFrame
print(train_with_class_df.head())

# Get unique ner_tag values from your processed DataFrame
ner_tags = train_df['ner_tag'].unique()  # Replace train_df with eval_df for the eval dataset

# Get the tags from the tag_list_df
valid_tags = tag_list_df['tag'].unique()

# Find anomalies (tags in ner_tag but not in valid_tags)
anomalies = set(ner_tags) - set(valid_tags)

if anomalies:
    print(f"Anomalous tags found: {anomalies}")
else:
    print("All ner_tag values are valid.")

# Get unique ner_tag values from your processed DataFrame
ner_tags = eval_df['ner_tag'].unique()

# Find anomalies (tags in ner_tag but not in valid_tags)
anomalies = set(ner_tags) - set(valid_tags)

if anomalies:
    print(f"Anomalous tags found: {anomalies}")
else:
    print("All ner_tag values are valid.")

# Define the anomaly sets from train and eval datasets
anomalous_tags_train = {'ORG_I', 'B', 'OBRN_B', 'DDEM', '__', 'MEA_BI', 'I', 'PER_I', 'B_D`TM'}
anomalous_tags_eval = {'ORG_I', 'B', 'ABB', 'LOC_I', '__'}

# Combine both sets into a single set of unique anomaly tags
combined_anomalous_tags = anomalous_tags_train.union(anomalous_tags_eval)

# Query rows in train_df with anomalous tags
anomalous_rows_train = train_df[train_df['ner_tag'].isin(combined_anomalous_tags)]

# Query rows in eval_df with anomalous tags
anomalous_rows_eval = eval_df[eval_df['ner_tag'].isin(combined_anomalous_tags)]

# Combine anomalous rows from both datasets
combined_anomalous_rows = pd.concat([anomalous_rows_train, anomalous_rows_eval])

# Display the anomalous rows
print("Anomalous rows from train_df:")
print(anomalous_rows_train.head(80))
print("\nAnomalous rows from eval_df:")
print(anomalous_rows_eval)
print("\nCombined anomalous rows:")
print(combined_anomalous_rows)

# Save the combined anomalous rows to a CSV file
combined_anomalous_rows.to_csv('combined_anomalous_rows.csv', index=False, encoding='utf-8')

print(anomalous_rows_train.head(50))
print(anomalous_rows_eval.head(50))

#Anomalous tags found: {'ORG_I', 'B','ABB', 'LOC_I', 'OBRN_B', 'DDEM', '__', 'MEA_BI', 'I', 'PER_I', 'B_D`TM'}
examples = train_df.groupby('ner_tag').apply(lambda x: x.iloc[0])

# Print an example word, its ner_tag, and corresponding class number
for _, row in examples.iterrows():
    tag = row['ner_tag']
    word = row['word']
    class_number = tag_list_df.get(tag, "Unknown")  # Use "Unknown" for tags not in tag_list_df
    print(f"Tag: {tag}, Class Number: {class_number}, Example Word: {word}")

# Define the replacement rules for anomalies
replacement_dict = {
    'ORG_I': 'I_ORG',
    'B': 'B_NAME',
    'OBRN_B': 'B_NUM',
    'MEA_BI': 'I_MEA',
    'PER_I': 'I_PER'
}

# Combine all anomalous tags from train and eval datasets
anomalous_tags_train = {'ORG_I', 'B', 'OBRN_B', 'DDEM', '__', 'MEA_BI', 'I', 'PER_I', 'B_D`TM'}
anomalous_tags_eval = {'ORG_I', 'B', 'ABB', 'LOC_I', '__'}

all_anomalous_tags = anomalous_tags_train.union(anomalous_tags_eval)

# Function to replace ner_tag based on replacement_dict
def fix_ner_tag(ner_tag):
    if ner_tag in replacement_dict:  # Replace if in the replacement dictionary
        return replacement_dict[ner_tag]
    elif ner_tag in all_anomalous_tags:  # Replace other anomalies with 'O'
        return 'O'
    else:  # Keep valid original classes unchanged
        return ner_tag

# Apply the replacement function to train_df and eval_df
train_df['ner_tag'] = train_df['ner_tag'].apply(fix_ner_tag)
eval_df['ner_tag'] = eval_df['ner_tag'].apply(fix_ner_tag)

# Print a summary of the changes
print("Unique tags in train_df after cleaning:", train_df['ner_tag'].unique())
print("Unique tags in eval_df after cleaning:", eval_df['ner_tag'].unique())

train_with_class_df = join_ner_with_class(train_df, tag_list_df)
eval_with_class_df = join_ner_with_class(eval_df, tag_list_df)
print(train_with_class_df.head())

print(train_with_class_df)
print(eval_with_class_df)

'''from typing import List

pad_token_label_id = 99

def map_custom_tokenized_sentence_to_ner_tokenizer(targeted_tokens: List[str], targeted_ners: List[int]):
  tokens = []
  label_ids = []

  for i, (word, label) in enumerate(zip(targeted_tokens, targeted_ners)):
    word_tokens = tokenizer.tokenize(word)
    if (
        word_tokens
    ):
      tokens.extend(word_tokens)
      else:
        word_tokens = tokenizer.tokenize(tokenizer.unk_taken)
        tokens.extend(word_tokens)
      label_ids.extend([label] + [pad_token_label_id] * (len(word_tokens) - 1))

  tokens_full = [tokenizer.cls_token] + tokens + [tokenizer.sep_token]
  label_ids_full = [-100] + lable_ids + [-100]

  return label_ids_full, tokens_full'''

print("Unique NER tags in train_df:", train_with_class_df['ner_tag'].unique())
print("Unique class values in train_df:", train_with_class_df['class'].unique())
print("Unique NER tags in eval_df:", eval_with_class_df['ner_tag'].unique())
print("Unique class values in eval_df:", eval_with_class_df['class'].unique())

def group_sentences(df):
    sentences = []
    sentence = {"tokens": [], "ner_tags": []}
    current_file_id = None

    for _, row in df.iterrows():
        file_id = row["folder_name_row"].split("_")[0]  # Extract file ID
        if current_file_id is None:
            current_file_id = file_id

        if file_id != current_file_id:  # New sentence
            sentences.append(sentence)
            sentence = {"tokens": [], "ner_tags": []}
            current_file_id = file_id

        # Append word and class label
        sentence["tokens"].append(row["word"])
        sentence["ner_tags"].append(row["class"])

    if sentence["tokens"]:  # Append the last sentence
        sentences.append(sentence)

    return sentences

# Group sentences for train and eval DataFrames
train_sentences = group_sentences(train_with_class_df)
eval_sentences = group_sentences(eval_with_class_df)

print(f"Number of sentences in train set: {len(train_sentences)}")
print(f"Number of sentences in eval set: {len(eval_sentences)}")

from datasets import Dataset

def create_hf_dataset(sentences):
    return Dataset.from_dict({
        "tokens": [sentence["tokens"] for sentence in sentences],
        "ner_tags": [sentence["ner_tags"] for sentence in sentences]
    })

# Create Hugging Face Datasets
train_dataset = create_hf_dataset(train_sentences)
eval_dataset = create_hf_dataset(eval_sentences)

print(train_dataset)
print(eval_dataset)

df = pd.DataFrame(train_dataset[:])
print(df)

print(df.head(50))

df

# Find the longest token list and its length
longest_token_list = max(df["tokens"], key=len)  # Find the longest token list
longest_length = len(longest_token_list)         # Calculate its length

# Print results
print("Longest token list:", longest_token_list)
print("Length of the longest token list:", longest_length)

def split_long_sentences(df, max_length=60):
    """
    Splits sentences in the dataset that exceed max_length tokens into smaller chunks.
    Ensures alignment between tokens and ner_tags, and handles boundary cases.

    Args:
        df (pd.DataFrame): Input DataFrame with 'tokens' and 'ner_tags' columns.
        max_length (int): Maximum length for each chunk.

    Returns:
        pd.DataFrame: A new DataFrame with split rows, maintaining alignment.
    """
    new_rows = []

    for _, row in df.iterrows():
        tokens = row["tokens"]
        ner_tags = row["ner_tags"]

        # Ensure tokens and ner_tags are aligned
        assert len(tokens) == len(ner_tags), "Tokens and NER tags are misaligned!"

        # Split tokens and ner_tags into chunks of size max_length
        for i in range(0, len(tokens), max_length):
            chunk_tokens = tokens[i:i + max_length]
            chunk_ner_tags = ner_tags[i:i + max_length]

            # Append each chunk as a new row
            new_rows.append({"tokens": chunk_tokens, "ner_tags": chunk_ner_tags})

    # Create a new DataFrame from the split rows
    split_df = pd.DataFrame(new_rows)

    # Validate split integrity
    original_size = sum(len(row["tokens"]) for _, row in df.iterrows())
    split_size = sum(len(row["tokens"]) for _, row in split_df.iterrows())
    assert original_size == split_size, "Mismatch in token counts after splitting!"

    return split_df

# Example usage
split_df = split_long_sentences(df, max_length=60)

# Check the result
print(f"Original dataset size: {len(df)}")
print(f"New dataset size after splitting: {len(split_df)}")
print(split_df.head())

split_df.head(50)

tag_list = tag_list_df["tag"].tolist()
from datasets import Dataset, Features, Sequence, Value, ClassLabel

def turn_df_to_dataset(df, tag_list):
    """
    Converts a DataFrame to a Hugging Face Dataset while mapping NER tags to a ClassLabel feature.

    Args:
        df (pd.DataFrame): DataFrame containing 'tokens' and 'ner_tags' columns.
        tag_list (list): List of all possible NER tags (e.g., ['O', 'B-PER', 'I-PER', ...]).

    Returns:
        Dataset: A Hugging Face Dataset with proper ClassLabel for ner_tags.
    """
    # Define features for the dataset
    feature_schema = Features({
        "tokens": Sequence(Value("string")),  # Sequence of strings for tokens
        "ner_tags": Sequence(ClassLabel(names=tag_list))  # Sequence of NER labels as ClassLabel
    })

    # Convert DataFrame rows into a list of dictionaries
    processed_data = df.to_dict(orient="records")  # [{'tokens': [...], 'ner_tags': [...]}, ...]

    # Convert to Hugging Face Dataset with defined features
    dataset = Dataset.from_list(processed_data)
    dataset = dataset.cast(feature_schema)  # Use Features object for casting

    return dataset

hf_dataset = turn_df_to_dataset(split_df, tag_list)

print(hf_dataset)

hf_dataset

eval_dataset

import transformers
print(transformers.__version__)

import accelerate
print(accelerate.__version__)  # Ensure it is 0.26.0 or higher
import seqeval

from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification
from seqeval.metrics import classification_report, f1_score

bilstm_crf_task = "ner"
bilstm_crf_model_checkpoint = "Geotrend/bert-base-th-cased"
bilstm_crf_BATCH_SIZE = 32
bilstm_crf_tokenizer = AutoTokenizer.from_pretrained(bilstm_crf_model_checkpoint)
bilstm_crf_model = AutoModelForTokenClassification.from_pretrained(bilstm_crf_model_checkpoint,
                                                                   num_labels=len(tag_list))

def tokenize_and_align_labels(datasets,
                              tokenizer=bilstm_crf_tokenizer,
                              SPECIAL_TOKENS = -100):

    # Tokenize the tokens column
    tokenized_inputs = tokenizer(datasets["tokens"],
                                 padding=True,
                                 truncation=True,
                                 is_split_into_words=True,
                                 return_tensors="pt")

    # Handle label alignment
    label_all_tokens = True
    aligned_labels = []
    original_labels = datasets["ner_tags"]


    for i, label in enumerate(datasets["ner_tags"]):

      # Map tokens to original word indices
      word_ids = tokenized_inputs.word_ids(batch_index = i)
      previous_word_idx = None
      label_ids = []

      for word_idx in word_ids:

        # Special tokens get -100
        if word_idx is None:
          label_ids.append(SPECIAL_TOKENS)

        # Assign label for the first token of the word
        elif word_idx != previous_word_idx:
          label_ids.append(label[word_idx])

        # Handle sub-tokens
        else:
          label_ids.append(label[word_idx] if label_all_tokens else SPECIAL_TOKENS)

        previous_word_idx = word_idx


      # Append aligned labels
      aligned_labels.append(label_ids)



    # Add labels to the tokenized inputs
    tokenized_inputs["labels"] = aligned_labels
    tokenized_inputs["original_ner_tags"] = original_labels


    return tokenized_inputs

train_dataset_padding = hf_dataset.map(tokenize_and_align_labels, batched=True)
eval_dataset_padding = eval_dataset.map(tokenize_and_align_labels, batched=True)

# Access features
features = train_dataset_padding.features.keys()

# Print top 3 entries for each feature
for feature in features:
    print(f"{feature}:")
    for i in range(1):  # Print the first 3 rows for this feature
        print(train_dataset_padding[i][feature])
    print()

args = TrainingArguments(
    "task-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",  # Save checkpoint every epoch
    learning_rate=1e-5,  # Smaller learning rate
    per_device_train_batch_size=bilstm_crf_BATCH_SIZE,
    per_device_eval_batch_size=bilstm_crf_BATCH_SIZE,
    num_train_epochs=2,  # Increase epochs
    weight_decay=0.01,
    load_best_model_at_end=True,  # Load the best model during evaluation
    metric_for_best_model="f1_macro",  # Use F1 score to evaluate
    logging_dir="./logs",  # Log directory for monitoring
    logging_steps=50  # Log progress every 50 steps
)
data_collator = DataCollatorForTokenClassification(bilstm_crf_tokenizer)

from sklearn.metrics import precision_recall_fscore_support

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_predictions = [
        [p for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    true_labels = [
        [l for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    # Flatten predictions and labels for macro computation
    flat_predictions = [item for sublist in true_predictions for item in sublist]
    flat_labels = [item for sublist in true_labels for item in sublist]

    # Compute precision, recall, f1 score for each class
    precision, recall, f1, _ = precision_recall_fscore_support(
        flat_labels, flat_predictions, average="macro"
    )

    return {
        "precision_macro": precision,
        "recall_macro": recall,
        "f1_macro": f1,
    }

from transformers import Trainer

trainer = Trainer(
    model=bilstm_crf_model,
    args=args,
    train_dataset=train_dataset_padding,
    eval_dataset=eval_dataset_padding,
    tokenizer=bilstm_crf_tokenizer,
    data_collator=data_collator,  # Handles padding dynamically
    compute_metrics=compute_metrics  # Evaluates F1 and other metrics
)

trainer.train()

def process_test_folder(folder_path, output_file):
    all_data = []

    # Iterate through all files in the folder
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.txt'):  # Ensure only .txt files are processed
            file_id = file_name.split('.')[0]  # Extract the file ID (without extension)

            # Read the file line by line
            with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:
                for idx, line in enumerate(file):
                    parts = line.strip().split('\t')  # Split the line into parts
                    if len(parts) >= 1:  # Ensure there's at least a word component
                        word = parts[0]  # Extract only the word
                        folder_name_row = f"{file_id}_{idx}"  # File ID with row index
                        all_data.append({
                            "folder_name_row": folder_name_row,
                            "word": word
                        })

    # Create a DataFrame from the collected data
    df = pd.DataFrame(all_data)

    # Save the DataFrame to a CSV file
    df.to_csv(output_file, index=False, encoding='utf-8')

    return df

# Example usage for test folders
test_folder = 'test/test'  # Update with your test folder path
output_file = 'processed_test.csv'

test_df = process_test_folder(test_folder, output_file)

print(test_df.head())

checkpoint_path = "task-ner/checkpoint-12142"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
model = AutoModelForTokenClassification.from_pretrained(checkpoint_path)

# Ensure the model is in evaluation mode
model.eval()

print("Model and tokenizer loaded successfully.")

id_to_tag = dict(zip(tag_list_df["class"], tag_list_df["tag"]))
tag_to_id = dict(zip(tag_list_df["tag"], tag_list_df["class"]))

print("Mappings created successfully.")

def group_test_sentences(df):
    sentences = []
    sentence = {"tokens": []}
    current_file_id = None

    for _, row in df.iterrows():
        file_id = row["folder_name_row"].split("_")[0]  # Extract file ID
        if current_file_id is None:
            current_file_id = file_id

        if file_id != current_file_id:  # New sentence
            sentences.append(sentence)
            sentence = {"tokens": []}
            current_file_id = file_id

        # Append word to tokens
        sentence["tokens"].append(row["word"])

    if sentence["tokens"]:  # Append the last sentence
        sentences.append(sentence)

    return pd.DataFrame(sentences)

def split_test_sentences(df, max_length=60):
    new_rows = []

    for _, row in df.iterrows():
        tokens = row["tokens"]

        # Split tokens into chunks of size max_length
        for i in range(0, len(tokens), max_length):
            chunk_tokens = tokens[i:i+max_length]
            new_rows.append({"tokens": chunk_tokens})

    return pd.DataFrame(new_rows)

from datasets import Dataset

def turn_test_df_to_dataset(df):
    return Dataset.from_pandas(df)

def tokenize_test_set(datasets, tokenizer):
    # Tokenize the tokens column
    tokenized_inputs = tokenizer(
        datasets["tokens"],
        padding=True,
        truncation=True,
        is_split_into_words=True,
        return_tensors="pt",
    )
    return tokenized_inputs

grouped_test_df = group_test_sentences(test_df)

# Step 2: Split long sentences
split_test_df = split_test_sentences(grouped_test_df, max_length=60)

# Step 3: Convert to Hugging Face Dataset
hf_test_dataset = turn_test_df_to_dataset(split_test_df)

# Step 4: Tokenize test dataset
tokenized_test_inputs = tokenize_test_set(hf_test_dataset, bilstm_crf_tokenizer)

model.eval()
with torch.no_grad():
    outputs = model(**tokenized_test_inputs)
logits = outputs.logits
predictions = torch.argmax(logits, dim=-1).tolist()

predicted_tags = []
for i, token_ids in enumerate(tokenized_test_inputs["input_ids"]):
    word_ids = tokenized_test_inputs.word_ids(batch_index=i)
    sentence_tags = []
    for j, word_id in enumerate(word_ids):
        if word_id is not None:  # Skip special tokens
            sentence_tags.append(id_to_tag[predictions[i][j]])
    predicted_tags.append(sentence_tags)

final_predictions = []
for tokens, tags in zip(split_test_df["tokens"], predicted_tags):
    for token, tag in zip(tokens, tags):
        final_predictions.append({"word": token, "predicted_tag": tag})

predictions_df = pd.DataFrame(final_predictions)
print(predictions_df.head(50))

predictions_df

aligned_df = test_df[["folder_name_row"]].iloc[:len(predictions_df)].copy()
aligned_df["predicted_tag"] = predictions_df["predicted_tag"]

# Step 2: Handle extra rows in test_df
if len(test_df) > len(predictions_df):
    extra_rows = test_df.iloc[len(predictions_df):].copy()
    extra_rows["predicted_tag"] = "O"  # Assign default predicted_tag for extra rows
    aligned_df = pd.concat([aligned_df, extra_rows[["folder_name_row", "predicted_tag"]]], ignore_index=True)

# Step 3: Ensure final DataFrame contains only required columns
aligned_df = aligned_df[["folder_name_row", "predicted_tag"]]

aligned_df

submission = pd.read_csv("sample_submission.csv")  # Replace with actual path
tag_list_df = pd.read_csv("tag_list.csv")  # Replace with actual path

# Create mapping from tag to class
tag_to_class = dict(zip(tag_list_df["tag"], tag_list_df["class"]))

# Filter aligned_df to include only rows present in the submission CSV
filtered_df = aligned_df[aligned_df["folder_name_row"].isin(submission["id"])].copy()

# Map predicted_tag to class
filtered_df["ne"] = filtered_df["predicted_tag"].map(tag_to_class)

# Ensure the result matches the order of the submission file
submission = submission[["id"]].merge(filtered_df, left_on="id", right_on="folder_name_row", how="left")

# Retain only the required columns: id and ne
submission = submission[["id", "ne"]]

submission.head(50)

predictions_df.head(50)

submission.to_csv("final_submission.csv", index=False, encoding="utf-8")

submission